{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "# SageMaker Example"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Build the container\n",
    "\n",
    "demo codes are in `app/`\n",
    "build and push the docker with following commands:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install  -U sagemaker boto3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!bash build_and_push_sglang.sh"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Deploy on SageMaker\n",
    "\n",
    "define the model and deploy on SageMaker\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### 3.1 Init SageMaker session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# !pip install boto3 sagemaker transformers\n",
    "import re\n",
    "import json\n",
    "import os,dotenv\n",
    "import boto3\n",
    "import sagemaker\n",
    "from sagemaker import Model\n",
    "\n",
    "\n",
    "dotenv.load_dotenv()\n",
    "print(os.environ)\n",
    "\n",
    "boto_sess = boto3.Session(\n",
    "    region_name='us-east-1'\n",
    ")\n",
    "\n",
    "sess = sagemaker.session.Session(boto_session=boto_sess)\n",
    "# role = sagemaker.get_execution_role()\n",
    "role = os.environ.get('role')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# role"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Prepare model file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Option 2: deploy vllm by model_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!tar czvf model.tar.gz model_tar/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "s3_code_prefix = f\"sagemaker_endpoint/sglang/\"\n",
    "bucket = sess.default_bucket() \n",
    "code_artifact = sess.upload_data(\"model.tar.gz\", bucket, s3_code_prefix)\n",
    "print(f\"S3 Code or Model tar ball uploaded to --- > {code_artifact}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### 3.3 Deploy model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### test deployment from s3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sagemaker\n",
    "import boto3\n",
    "sm_client = boto3.client(service_name=\"sagemaker\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a SageMaker model\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(llama3-8b-scale-to-zero-autoscaling)[https://github.com/aws-samples/sagemaker-genai-hosting-examples/blob/main/scale-to-zero-endpoint/llama3-8b-scale-to-zero-autoscaling.ipynb]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "CONTAINER='434444145045.dkr.ecr.us-east-1.amazonaws.com/sagemaker_endpoint/sglang:v0.4.3.post2-cu124'\n",
    "# CONTAINER='434444145045.dkr.ecr.us-east-1.amazonaws.com/sagemaker_endpoint/vllm:v0.7.2'\n",
    "model_path = \"s3://sagemaker-us-east-1-434444145045/Qwen2-1-5B-Instruct/6d0410c634ea438fa5018072e84c10a6/finetuned_model_merged/\"\n",
    "model_id = 'Qwen/Qwen2-1.5B-Instruct'\n",
    "base_name = sagemaker.utils.name_from_base(\"sagemaker\")\n",
    "sm_client = boto3.client(service_name=\"sagemaker\")\n",
    "model_name = base_name +\"-model\"\n",
    "endpoint_name = base_name+\"-endpoint\"\n",
    "component_name = base_name+\"-component\"\n",
    "endpoint_config_name =  base_name+\"-config\"\n",
    "env={\n",
    "    \"HF_MODEL_ID\": model_id,\n",
    "    \"S3_MODEL_PATH\":model_path,\n",
    "}\n",
    "container_config = {\n",
    "    'Image': CONTAINER,\n",
    "    'ModelDataUrl': code_artifact,\n",
    "    'Environment': env\n",
    "}\n",
    "\n",
    "print(model_name)\n",
    "print(endpoint_name)\n",
    "print(component_name)\n",
    "print(endpoint_config_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# endpoint_name = \"sagemaker-2025-03-01-11-30-13-897-endpoint\"\n",
    "# component_name = \"sagemaker-2025-03-01-11-30-13-897-component\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "response = sm_client.create_model(\n",
    "    ModelName=model_name,\n",
    "    ExecutionRoleArn=role,\n",
    "    PrimaryContainer=container_config\n",
    ")\n",
    "\n",
    "print(f\"Model created: {response['ModelArn']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a SageMaker endpoint configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "instance_type = \"ml.g5.2xlarge\"\n",
    "model_data_download_timeout_in_seconds = 1200\n",
    "container_startup_health_check_timeout_in_seconds = 1200\n",
    "\n",
    "min_instance_count = 0 # Minimum instance must be set to 0\n",
    "max_instance_count = 2\n",
    "\n",
    "sm_client.create_endpoint_config(\n",
    "    EndpointConfigName=endpoint_config_name,\n",
    "    ExecutionRoleArn=role,\n",
    "    ProductionVariants=[\n",
    "        {\n",
    "            \"VariantName\": \"AllTraffic\",\n",
    "            \"InstanceType\": instance_type,\n",
    "            \"InitialInstanceCount\": 1,\n",
    "            \"ModelDataDownloadTimeoutInSeconds\": model_data_download_timeout_in_seconds,\n",
    "            \"ContainerStartupHealthCheckTimeoutInSeconds\": container_startup_health_check_timeout_in_seconds,\n",
    "            \"ManagedInstanceScaling\": {\n",
    "                \"Status\": \"ENABLED\",\n",
    "                \"MinInstanceCount\": min_instance_count,\n",
    "                \"MaxInstanceCount\": max_instance_count,\n",
    "            },\n",
    "            \"RoutingConfig\": {\"RoutingStrategy\": \"LEAST_OUTSTANDING_REQUESTS\"},\n",
    "        }\n",
    "    ],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create the SageMaker endpoint\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sm_client.create_endpoint(\n",
    "    EndpointName=endpoint_name,\n",
    "    EndpointConfigName=endpoint_config_name,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "resp = sm_client.describe_endpoint(EndpointName=endpoint_name)\n",
    "status = resp[\"EndpointStatus\"]\n",
    "print(\"Status: \" + status)\n",
    "\n",
    "while status == \"Creating\":\n",
    "    time.sleep(60)\n",
    "    resp = sm_client.describe_endpoint(EndpointName=endpoint_name)\n",
    "    status = resp[\"EndpointStatus\"]\n",
    "    print(\"Status: \" + status)\n",
    "\n",
    "print(\"Arn: \" + resp[\"EndpointArn\"])\n",
    "print(\"Status: \" + status)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t1 = time.time()\n",
    "sm_client.create_inference_component(\n",
    "    InferenceComponentName=component_name,\n",
    "    EndpointName=endpoint_name,\n",
    "    VariantName=\"AllTraffic\",\n",
    "    Specification={\n",
    "        \"ModelName\": model_name,\n",
    "        \"ComputeResourceRequirements\": {\n",
    "\t\t    \"NumberOfAcceleratorDevicesRequired\": 1, \n",
    "\t\t\t#\"NumberOfCpuCoresRequired\": 2, \n",
    "\t\t\t\"MinMemoryRequiredInMb\": 1024*8\n",
    "\t    }\n",
    "    },\n",
    "    RuntimeConfig={\"CopyCount\": 1},\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Test\n",
    "\n",
    "you can invoke your model with SageMaker SDK"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1 Message api non-stream mode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_inference_component_status(inference_component_name):\n",
    "    sagemaker_client = boto3.client('sagemaker')\n",
    "    \n",
    "    try:\n",
    "        response = sagemaker_client.describe_inference_component(\n",
    "            InferenceComponentName=inference_component_name\n",
    "        )\n",
    "        # print(response)\n",
    "        status = response['InferenceComponentStatus']\n",
    "        print(f\"Inference Component Status: {status}\")\n",
    "        \n",
    "        if status == 'InService':\n",
    "            print(\"Inference component has been successfully deployed\")\n",
    "            return True\n",
    "        elif status in ['Creating', 'Updating']:\n",
    "            print(\"Inference component is still being deployed...\")\n",
    "            return False\n",
    "        else:\n",
    "            print(f\"Inference component deployment failed with status: {status}\")\n",
    "            # You might want to check the FailureReason if available\n",
    "            if 'FailureReason' in response:\n",
    "                print(f\"Failure reason: {response['FailureReason']}\")\n",
    "            return True\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"Error checking inference component status: {str(e)}\")\n",
    "        return True\n",
    "    \n",
    "while True:\n",
    "    if  check_inference_component_status(component_name):\n",
    "        print(f\"Inference Component is ready:{time.time()-t1:.1f} s\")\n",
    "        break\n",
    "    time.sleep(30)\n",
    "    \n",
    "from sagemaker.predictor import retrieve_default \n",
    "from sagemaker import Predictor\n",
    "from sagemaker import serializers, deserializers\n",
    "# predictor = retrieve_default(endpoint_name, sagemaker_session=sess) \n",
    "\n",
    "predictor = Predictor(\n",
    "            endpoint_name=endpoint_name,\n",
    "            sagemaker_session=sess,\n",
    "            serializer=serializers.JSONSerializer(),\n",
    "            component_name=component_name\n",
    "        )\n",
    "\n",
    "payload = {\n",
    "    \"messages\": [\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": \"who are you\"\n",
    "    }\n",
    "    ],\n",
    "    \"model\":\"qwen\",\n",
    "    \"max_tokens\": 1024,\n",
    "    \"stream\": False\n",
    "}\n",
    "response = predictor.predict(payload) \n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# component_name = \"sagemaker-2025-03-01-00-39-49-809-component\"\n",
    "# endpoint_name = \"sagemaker-2025-03-01-00-39-49-809-endpoint\"\n",
    "runtime = boto3.client('runtime.sagemaker',region_name='us-east-1')\n",
    "payload = {\n",
    "    \"messages\": [\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": \"who are you\"\n",
    "    }\n",
    "    ],\n",
    "    \"model\":\"qwen\",\n",
    "    \"max_tokens\": 1024,\n",
    "    \"stream\": False\n",
    "}\n",
    "response = runtime.invoke_endpoint(\n",
    "    EndpointName=endpoint_name,\n",
    "    InferenceComponentName=component_name,\n",
    "    ContentType='application/json',\n",
    "    Body=json.dumps(payload)\n",
    ")\n",
    "\n",
    "print(json.loads(response['Body'].read())[\"choices\"][0][\"message\"][\"content\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### 4.2 Message api stream mode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "payload = {\n",
    "    \"messages\": [\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": \"Write a quick sort in python\"\n",
    "    }\n",
    "    ],\n",
    "    \"model\":\"custome\",\n",
    "    \"max_tokens\": 4096,\n",
    "    \"stream\": True\n",
    "}\n",
    "\n",
    "response = runtime.invoke_endpoint_with_response_stream(\n",
    "    EndpointName=endpoint_name,\n",
    "    InferenceComponentName=component_name,\n",
    "    ContentType='application/json',\n",
    "    Body=json.dumps(payload)\n",
    ")\n",
    "\n",
    "buffer = \"\"\n",
    "for t in response['Body']:\n",
    "    buffer += t[\"PayloadPart\"][\"Bytes\"].decode()\n",
    "    last_idx = 0\n",
    "    for match in re.finditer(r'^data:\\s*(.+?)(\\n\\n)', buffer):\n",
    "        try:\n",
    "            data = json.loads(match.group(1).strip())\n",
    "            last_idx = match.span()[1]\n",
    "            print(data[\"choices\"][0][\"delta\"][\"content\"], end=\"\")\n",
    "        except (json.JSONDecodeError, KeyError, IndexError) as e:\n",
    "            pass\n",
    "    buffer = buffer[last_idx:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3 Register a new autoscaling target\n",
    "After you create your SageMaker endpoint and inference components, you register a new auto scaling target for Application Auto Scaling. In the following code block, you set MinCapacity to 0, which is required for your endpoint to scale down to zero\n",
    "\n",
    "https://aws.amazon.com/cn/blogs/machine-learning/unlock-cost-savings-with-the-new-scale-down-to-zero-feature-in-amazon-sagemaker-inference/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "aas_client = sess.boto_session.client(\"application-autoscaling\")\n",
    "cloudwatch_client = sess.boto_session.client(\"cloudwatch\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 这一步必须，先注册resource_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Autoscaling parameters\n",
    "resource_id = f\"inference-component/{component_name}\"\n",
    "service_namespace = \"sagemaker\"\n",
    "scalable_dimension = \"sagemaker:inference-component:DesiredCopyCount\"\n",
    "\n",
    "min_copy_count = 0\n",
    "max_copy_count = 4\n",
    "\n",
    "aas_client.register_scalable_target(\n",
    "    ServiceNamespace=service_namespace,\n",
    "    ResourceId=resource_id,\n",
    "    ScalableDimension=scalable_dimension,\n",
    "    MinCapacity=min_copy_count,\n",
    "    MaxCapacity=max_copy_count,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- After you have registered your new scalable target, the next step is to define your target tracking policy. In the following code example, we set the TargetValue to 5. This setting instructs the auto scaling system to increase capacity when the number of concurrent requests per model reaches or exceeds 5."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aas_client.describe_scalable_targets(\n",
    "    ServiceNamespace=service_namespace,\n",
    "    ResourceIds=[resource_id],\n",
    "    ScalableDimension=scalable_dimension,\n",
    ")\n",
    "\n",
    "# The policy name for the target traking policy\n",
    "target_tracking_policy_name = f\"Target-tracking-policy-{component_name}\"\n",
    "\n",
    "aas_client.put_scaling_policy(\n",
    "    PolicyName=target_tracking_policy_name,\n",
    "    PolicyType=\"TargetTrackingScaling\",\n",
    "    ServiceNamespace=service_namespace,\n",
    "    ResourceId=resource_id,\n",
    "    ScalableDimension=scalable_dimension,\n",
    "    TargetTrackingScalingPolicyConfiguration={\n",
    "        \"PredefinedMetricSpecification\": {\n",
    "            \"PredefinedMetricType\": \"SageMakerInferenceComponentConcurrentRequestsPerCopyHighResolution\",\n",
    "        },\n",
    "        # Low TPS + load TPS\n",
    "        \"TargetValue\": 5,  # you need to adjust this value based on your use case\n",
    "        \"ScaleInCooldown\": 180,  # default 300\n",
    "        \"ScaleOutCooldown\": 180,  # default 300\n",
    "    },\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scale out from zero policy (step scaling policy )\n",
    "To enable your endpoint to scale out from zero instances, do the following:\n",
    "\n",
    "Configure Step Scaling Policy\n",
    "Create a step scaling policy that defines when and how to scale out from zero. This policy will add 1 model copy when triggered, enabling SageMaker to provision the instances required to handle incoming requests after being idle. The following shows you how to define a step scaling policy. Here we have configured to scale out from 0 to 1 model copy (\"ScalingAdjustment\": 1), depending on your use case you can adjust ScalingAdjustment as required.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#The policy name for the step scaling policy\n",
    "\n",
    "# Autoscaling parameters\n",
    "resource_id = f\"inference-component/{component_name}\"\n",
    "service_namespace = \"sagemaker\"\n",
    "scalable_dimension = \"sagemaker:inference-component:DesiredCopyCount\"\n",
    "step_scaling_policy_name = f\"Step-scaling-policy-{component_name}\"\n",
    "\n",
    "aas_client.put_scaling_policy(\n",
    "    PolicyName=step_scaling_policy_name,\n",
    "    PolicyType=\"StepScaling\",\n",
    "    ServiceNamespace=service_namespace,\n",
    "    ResourceId=resource_id,\n",
    "    ScalableDimension=scalable_dimension,\n",
    "    StepScalingPolicyConfiguration={\n",
    "        \"AdjustmentType\": \"ChangeInCapacity\",\n",
    "        \"MetricAggregationType\": \"Maximum\",\n",
    "        \"Cooldown\": 60,\n",
    "        \"StepAdjustments\":\n",
    "          [\n",
    "             {\n",
    "               \"MetricIntervalLowerBound\": 0,\n",
    "               \"ScalingAdjustment\": 1\n",
    "             }\n",
    "          ]\n",
    "    },\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "resp = aas_client.describe_scaling_policies(\n",
    "    PolicyNames=[step_scaling_policy_name,target_tracking_policy_name],\n",
    "    ServiceNamespace=service_namespace,\n",
    "    ResourceId=resource_id,\n",
    "    ScalableDimension=scalable_dimension,\n",
    ")\n",
    "step_scaling_policy_arn = resp['ScalingPolicies'][0]['PolicyARN']\n",
    "print(f\"step_scaling_policy_arn: {step_scaling_policy_arn}\")\n",
    "print(resp['ScalingPolicies'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create the CloudWatch alarm that will trigger our policy\n",
    "Finally, create a CloudWatch alarm with the metric NoCapacityInvocationFailures. When triggered, the alarm initiates the previously defined scaling policy. For more information about the NoCapacityInvocationFailures metric, see documentation.\n",
    "\n",
    "We have also set the following:  \n",
    "\n",
    "- EvaluationPeriods to 1  \n",
    "- DatapointsToAlarm to 1  \n",
    "- ComparisonOperator to GreaterThanOrEqualToThreshold  \n",
    "- This results in 1 min waiting for the step scaling policy to trigger  \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The alarm name for the step scaling alarm\n",
    "step_scaling_alarm_name = f\"step-scaling-alarm-scale-to-zero-aas-{component_name}\"\n",
    "\n",
    "cloudwatch_client.put_metric_alarm(\n",
    "    AlarmName=step_scaling_alarm_name,\n",
    "    AlarmActions=[step_scaling_policy_arn],  # Replace with your actual ARN\n",
    "    MetricName='NoCapacityInvocationFailures',\n",
    "    Namespace='AWS/SageMaker',\n",
    "    Statistic='Maximum',\n",
    "    Dimensions=[\n",
    "        {\n",
    "            'Name': 'InferenceComponentName',\n",
    "            'Value': component_name  # Replace with actual InferenceComponentName\n",
    "        }\n",
    "    ],\n",
    "    Period=30, # 定义了 CloudWatch 收集和聚合指标数据的时间间隔，CloudWatch 支持的最小 Period 值通常为 10 或 60 秒，取决于指标类型和监控级别\n",
    "    EvaluationPeriods=1, #定义了在多少个连续的数据点中需要满足条件才会触发警报，=1 表示只需要评估 1 个时间段（即 30 秒，由 Period 定义）\n",
    "    DatapointsToAlarm=1, #表示在 EvaluationPeriods 中需要满足阈值条件的数据点数量，=1 表示在评估的 1 个时间段内，只要有 1 个数据点满足条件就触发警报\n",
    "    Threshold=1, #表示当 NoCapacityInvocationFailures 指标值大于或等于 1 时触发警报\n",
    "    ComparisonOperator='GreaterThanOrEqualToThreshold',\n",
    "    TreatMissingData='missing' #缺失的数据点不会触发警报状态的任何变化，notBreaching将缺失的数据点视为\"良好\"或\"未违反阈值\"，breaching将缺失的数据点视为\"违反阈值\"，ignore保持当前警报状态不变，直到有新数据点出现\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### check updating status"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "while True:\n",
    "    if  check_inference_component_status(component_name):\n",
    "        break\n",
    "    time.sleep(30)\n",
    "start_time = time.time()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test the solution\n",
    "Notice the MinInstanceCount: 0 setting in the Endpoint configuration, which allows the endpoint to scale down to zero instances. With the scaling policy, CloudWatch alarm, and minimum instances set to zero, your SageMaker Inference Endpoint will now be able to automatically scale down to zero instances when not in use, helping you optimize your costs and resource utilization.\n",
    "\n",
    "### IC copy count scales in to zero\n",
    "We'll pause for a few minutes without making any invocations to our model. Based on our target tracking policy, when our SageMaker endpoint doesn't receive requests for about 10 to 15 minutes, it will automatically scale down to zero the number of model copies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "# time.sleep(900)\n",
    "while True:\n",
    "    desc = sm_client.describe_inference_component(InferenceComponentName=component_name)\n",
    "    status = desc[\"InferenceComponentStatus\"]\n",
    "    print(status)\n",
    "    sys.stdout.flush()\n",
    "    if status not in [\"InService\", \"Failed\"]:\n",
    "        break\n",
    "    time.sleep(30)\n",
    "\n",
    "total_time = time.time() - start_time\n",
    "print(f\"\\nTotal time taken: {total_time:.2f} seconds ({total_time/60:.2f} minutes)\")\n",
    "\n",
    "desc = sm_client.describe_inference_component(InferenceComponentName=component_name)\n",
    "print(desc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Endpoint's instances scale in to zero\n",
    "After 10 additional minutes of inactivity, SageMaker automatically terminates all underlying instances of the endpoint, eliminating all associated costs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sess.wait_for_endpoint(endpoint_name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- After 10 additional minutes of inactivity, SageMaker automatically stops all underlying instances of the endpoint, eliminating all associated instance costs.\n",
    "\n",
    "If we try to invoke our endpoint while instances are scaled down to zero, we get a validation error:  \n",
    "`An error occurred (ValidationError) when calling the InvokeEndpoint operation: Inference Component has no capacity to process this request. ApplicationAutoScaling may be in-progress (if configured) or try to increase the capacity by invoking UpdateInferenceComponentRuntimeConfig API.`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.predictor import retrieve_default \n",
    "from sagemaker import Predictor\n",
    "from sagemaker import serializers, deserializers\n",
    "# predictor = retrieve_default(endpoint_name, sagemaker_session=sess) \n",
    "\n",
    "predictor = Predictor(\n",
    "            endpoint_name=endpoint_name,\n",
    "            sagemaker_session=sess,\n",
    "            serializer=serializers.JSONSerializer(),\n",
    "            component_name=component_name\n",
    "        )\n",
    "\n",
    "payload = {\n",
    "    \"messages\": [\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": \"who are you\"\n",
    "    }\n",
    "    ],\n",
    "    \"model\":\"qwen\",\n",
    "    \"max_tokens\": 1024,\n",
    "    \"stream\": False\n",
    "}\n",
    "response = predictor.predict(payload) \n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## test for scaling up\n",
    "t1 = time.time()\n",
    "i = 0\n",
    "while True:\n",
    "    predictor.predict(payload) \n",
    "    i += 1\n",
    "    if i % 10 == 0:\n",
    "        print(response)\n",
    "        print(f\"---{i}----time: {(time.time() - t1):.1f}\")\n",
    "    if i == 100000:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.4 Delete"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!aws sagemaker delete-inference-component --inference-component-name {component_name}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!aws sagemaker delete-endpoint --endpoint-name {endpoint_name}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!aws sagemaker delete-endpoint-config --endpoint-config-name {endpoint_config_name}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!aws sagemaker delete-model --model-name {model_name}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py311",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
