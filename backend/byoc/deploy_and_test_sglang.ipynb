{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "# SageMaker Example"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Build the container\n",
    "\n",
    "demo codes are in `app/`\n",
    "build and push the docker with following commands:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "set -e\n",
      "\n",
      "# This script shows how to build the Docker image and push it to ECR to be ready for use\n",
      "# by SageMaker.\n",
      "\n",
      "# The argument to this script is the region name. \n",
      "# 尝试使用 IMDSv2 获取 token\n",
      "TOKEN=$(curl -X PUT \"http://169.254.169.254/latest/api/token\" -H \"X-aws-ec2-metadata-token-ttl-seconds: 21600\")\n",
      "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
      "                                 Dload  Upload   Total   Spent    Left  Speed\n",
      "100    56  100    56    0     0  45197      0 --:--:-- --:--:-- --:--:-- 56000\n",
      "\n",
      "# Get the current region and write it to the backend .env file\n",
      "region=$(curl -H \"X-aws-ec2-metadata-token: $TOKEN\" -s http://169.254.169.254/latest/meta-data/placement/region)\n",
      "# region=$(aws configure get region)\n",
      "suffix=\"com\"\n",
      "\n",
      "if [[ $region =~ ^cn ]]; then\n",
      "    suffix=\"com.cn\"\n",
      "fi\n",
      "\n",
      "# Get the account number associated with the current IAM credentials\n",
      "account=$(aws sts  get-caller-identity --query Account --output text)\n",
      "\n",
      "SGL_VERSION=latest\n",
      "inference_image=sagemaker_endpoint/sglang\n",
      "inference_fullname=${account}.dkr.ecr.${region}.amazonaws.${suffix}/${inference_image}:${SGL_VERSION}\n",
      "\n",
      "# If the repository doesn't exist in ECR, create it.\n",
      "aws  ecr describe-repositories --repository-names \"${inference_image}\" --region ${region} || aws ecr create-repository --repository-name \"${inference_image}\" --region ${region}\n",
      "{\n",
      "    \"repositories\": [\n",
      "        {\n",
      "            \"repositoryArn\": \"arn:aws:ecr:us-east-1:434444145045:repository/sagemaker_endpoint/sglang\",\n",
      "            \"registryId\": \"434444145045\",\n",
      "            \"repositoryName\": \"sagemaker_endpoint/sglang\",\n",
      "            \"repositoryUri\": \"434444145045.dkr.ecr.us-east-1.amazonaws.com/sagemaker_endpoint/sglang\",\n",
      "            \"createdAt\": 1739108011.378,\n",
      "            \"imageTagMutability\": \"MUTABLE\",\n",
      "            \"imageScanningConfiguration\": {\n",
      "                \"scanOnPush\": false\n",
      "            },\n",
      "            \"encryptionConfiguration\": {\n",
      "                \"encryptionType\": \"AES256\"\n",
      "            }\n",
      "        }\n",
      "    ]\n",
      "}\n",
      "\n",
      "if [ $? -ne 0 ]\n",
      "then\n",
      "    aws  ecr create-repository --repository-name \"${inference_image}\" --region ${region}\n",
      "fi\n",
      "\n",
      "# Get the login command from ECR and execute it directly\n",
      "aws  ecr get-login-password --region $region | docker login --username AWS --password-stdin $account.dkr.ecr.$region.amazonaws.${suffix}\n",
      "WARNING! Your password will be stored unencrypted in /home/ubuntu/.docker/config.json.\n",
      "Configure a credential helper to remove this warning. See\n",
      "https://docs.docker.com/engine/reference/commandline/login/#credentials-store\n",
      "\n",
      "Login Succeeded\n",
      "\n",
      "aws ecr set-repository-policy \\\n",
      "    --repository-name \"${inference_image}\" \\\n",
      "    --policy-text \"file://ecr-policy.json\" \\\n",
      "    --region ${region}\n",
      "{\n",
      "    \"registryId\": \"434444145045\",\n",
      "    \"repositoryName\": \"sagemaker_endpoint/sglang\",\n",
      "    \"policyText\": \"{\\n  \\\"Version\\\" : \\\"2008-10-17\\\",\\n  \\\"Statement\\\" : [ {\\n    \\\"Sid\\\" : \\\"new statement\\\",\\n    \\\"Effect\\\" : \\\"Allow\\\",\\n    \\\"Principal\\\" : \\\"*\\\",\\n    \\\"Action\\\" : [ \\\"ecr: CompleteLayerUpload\\\", \\\"ecr: InitiateLayerUpload\\\", \\\"ecr: ListImages\\\", \\\"ecr:BatchCheckLayerAvailability\\\", \\\"ecr:BatchGetImage\\\", \\\"ecr:DescribeImages\\\", \\\"ecr:DescribeRepositories\\\", \\\"ecr:GetDownloadUrlForLayer\\\" ]\\n  } ]\\n}\"\n",
      "}\n",
      "\n",
      "# Build the docker image locally with the image name and then push it to ECR\n",
      "# with the full name.\n",
      "\n",
      "docker build  --build-arg SGL_VERSION=${SGL_VERSION} -t ${inference_image}:${SGL_VERSION}  -f Dockerfile.sglang . \n",
      "\u001b[1A\u001b[1B\u001b[0G\u001b[?25l[+] Building 0.0s (0/2)                                          docker:default\n",
      "\u001b[?25h\u001b[1A\u001b[0G\u001b[?25l[+] Building 0.1s (10/11)                                        docker:default\n",
      "\u001b[34m => [internal] load .dockerignore                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2B                                            0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load build definition from Dockerfile.sglang                0.0s\n",
      "\u001b[0m\u001b[34m => => transferring dockerfile: 953B                                       0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load metadata for docker.io/lmsysorg/sglang:latest          0.1s\n",
      "\u001b[0m\u001b[34m => [1/6] FROM docker.io/lmsysorg/sglang:latest@sha256:c693c32445ea938e11  0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load build context                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 103B                                          0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [2/6] WORKDIR /app                                              0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [3/6] COPY app_sglang/ /app                                     0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [4/6] COPY requirements.txt /app                                0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [5/6] RUN pip install -r requirements.txt                       0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [6/6] RUN export PYTHON_SITEPACKAGES=`python3 -c \"import sglan  0.0s\n",
      "\u001b[0m => exporting to image                                                     0.0s\n",
      "\u001b[34m => => exporting layers                                                    0.0s\n",
      "\u001b[0m\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 0.1s (11/11) FINISHED                               docker:default\n",
      "\u001b[34m => [internal] load .dockerignore                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2B                                            0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load build definition from Dockerfile.sglang                0.0s\n",
      "\u001b[0m\u001b[34m => => transferring dockerfile: 953B                                       0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load metadata for docker.io/lmsysorg/sglang:latest          0.1s\n",
      "\u001b[0m\u001b[34m => [1/6] FROM docker.io/lmsysorg/sglang:latest@sha256:c693c32445ea938e11  0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load build context                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 103B                                          0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [2/6] WORKDIR /app                                              0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [3/6] COPY app_sglang/ /app                                     0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [4/6] COPY requirements.txt /app                                0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [5/6] RUN pip install -r requirements.txt                       0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [6/6] RUN export PYTHON_SITEPACKAGES=`python3 -c \"import sglan  0.0s\n",
      "\u001b[0m\u001b[34m => exporting to image                                                     0.0s\n",
      "\u001b[0m\u001b[34m => => exporting layers                                                    0.0s\n",
      "\u001b[0m\u001b[34m => => writing image sha256:0b487ad1e21e0f0663340ac26742a8d2601830a17f9db  0.0s\n",
      "\u001b[0m\u001b[34m => => naming to docker.io/sagemaker_endpoint/sglang:latest                0.0s\n",
      "\u001b[0m\u001b[?25h\n",
      "docker tag ${inference_image}:${SGL_VERSION} ${inference_fullname}\n",
      "\n",
      "docker push ${inference_fullname}\n",
      "The push refers to repository [434444145045.dkr.ecr.us-east-1.amazonaws.com/sagemaker_endpoint/sglang]\n",
      "\n",
      "\u001b[1B44721b19: Preparing \n",
      "\u001b[1B012919ce: Preparing \n",
      "\u001b[1Bbad29619: Preparing \n",
      "\u001b[1B53b18379: Preparing \n",
      "\u001b[1B266298fb: Preparing \n",
      "\u001b[1B9a3b8aa7: Preparing \n",
      "\u001b[1Bc87fedfc: Preparing \n",
      "\u001b[1B7495d60d: Preparing \n",
      "\u001b[1B9895f7cd: Preparing \n",
      "\u001b[1B8bbccdd1: Preparing \n",
      "\u001b[1B73680ef3: Preparing \n",
      "\u001b[1B14258878: Preparing \n",
      "\u001b[1B9ed2f631: Preparing \n",
      "\u001b[1Bda6ba3ae: Preparing \n",
      "\u001b[1B70b9a284: Preparing \n",
      "\u001b[1B88486056: Preparing \n",
      "\u001b[1B7c97ed4d: Preparing \n",
      "\u001b[1B3c3b8514: Preparing \n",
      "\u001b[1Bb7e2d072: Preparing \n",
      "\u001b[1B8f6fcd6a: Preparing \n",
      "\u001b[1Bded77c0c: Layer already exists \u001b[16A\u001b[2K\u001b[15A\u001b[2K\u001b[11A\u001b[2K\u001b[5A\u001b[2K\u001b[2A\u001b[2Klatest: digest: sha256:273074cde17b788ebc0db084d0f7eb256ad01f6cd5417d125eeabc02b9a1e44b size: 4721\n",
      "# 删除 .env 文件中的 sglang_image= 这一行\n",
      "sed -i '/^sglang_image=/d' /home/ubuntu/llm_model_hub/backend/.env\n",
      "sed: can't read /home/ubuntu/llm_model_hub/backend/.env: No such file or directory\n"
     ]
    }
   ],
   "source": [
    "!bash build_and_push_sglang.sh"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Deploy on SageMaker\n",
    "\n",
    "define the model and deploy on SageMaker\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### 3.1 Init SageMaker session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sagemaker.config INFO - Not applying SDK defaults from location: /etc/xdg/sagemaker/config.yaml\n",
      "sagemaker.config INFO - Not applying SDK defaults from location: /home/ubuntu/.config/sagemaker/config.yaml\n",
      "environ({'USER': 'ubuntu', 'SSH_CLIENT': '72.21.198.67 21575 22', 'XDG_SESSION_TYPE': 'tty', 'SHLVL': '2', 'HOME': '/home/ubuntu', 'OLDPWD': '/home/ubuntu/.vscode-server', 'SSL_CERT_FILE': '/usr/lib/ssl/cert.pem', 'DBUS_SESSION_BUS_ADDRESS': 'unix:path=/run/user/1000/bus', 'LOGNAME': 'ubuntu', '_': '/home/ubuntu/workspace/llm_model_hub/miniconda3/envs/py311/bin/python', 'XDG_SESSION_CLASS': 'user', 'XDG_SESSION_ID': '32521', 'VSCODE_CLI_REQUIRE_TOKEN': 'b6cfbca6-9b17-4767-9219-e97b4b919005', 'PATH': '/home/ubuntu/workspace/llm_model_hub/miniconda3/envs/py311/bin:/home/ubuntu/.vscode-server/cli/servers/Stable-e54c774e0add60467559eb0d1e229c6452cf8447/server/bin/remote-cli:/home/ubuntu/.local/bin:/home/ubuntu/workspace/llm_model_hub/miniconda3/envs/py311/bin:/home/ubuntu/workspace/llm_model_hub/miniconda3/condabin:/home/ubuntu/.local/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/usr/games:/usr/local/games:/snap/bin', 'VSCODE_AGENT_FOLDER': '/home/ubuntu/.vscode-server', 'XDG_RUNTIME_DIR': '/run/user/1000', 'SSL_CERT_DIR': '/usr/lib/ssl/certs', 'LANG': 'C.UTF-8', 'SHELL': '/bin/bash', 'PWD': '/home/ubuntu', 'SSH_CONNECTION': '72.21.198.67 21575 172.31.38.107 22', 'XDG_DATA_DIRS': '/usr/local/share:/usr/share:/var/lib/snapd/desktop', 'VSCODE_CWD': '/home/ubuntu', 'VSCODE_NLS_CONFIG': '{\"userLocale\":\"en\",\"osLocale\":\"en\",\"resolvedLanguage\":\"en\",\"defaultMessagesFile\":\"/home/ubuntu/.vscode-server/cli/servers/Stable-e54c774e0add60467559eb0d1e229c6452cf8447/server/out/nls.messages.json\",\"locale\":\"en\",\"availableLanguages\":{}}', 'VSCODE_HANDLES_SIGPIPE': 'true', 'CONDA_EXE': '/home/ubuntu/workspace/llm_model_hub/miniconda3/bin/conda', '_CE_M': '', 'CONDA_PREFIX': '/home/ubuntu/workspace/llm_model_hub/miniconda3/envs/py311', 'LS_COLORS': '', 'CONDA_PROMPT_MODIFIER': '(py311) ', 'LESSCLOSE': '/usr/bin/lesspipe %s %s', '_CE_CONDA': '', 'LESSOPEN': '| /usr/bin/lesspipe %s', 'CONDA_SHLVL': '2', 'CONDA_PYTHON_EXE': '/home/ubuntu/workspace/llm_model_hub/miniconda3/bin/python', 'CONDA_DEFAULT_ENV': 'py311', 'VSCODE_ESM_ENTRYPOINT': 'vs/workbench/api/node/extensionHostProcess', 'VSCODE_HANDLES_UNCAUGHT_ERRORS': 'true', 'BROWSER': '/home/ubuntu/.vscode-server/cli/servers/Stable-e54c774e0add60467559eb0d1e229c6452cf8447/server/bin/helpers/browser.sh', 'ELECTRON_RUN_AS_NODE': '1', 'VSCODE_IPC_HOOK_CLI': '/run/user/1000/vscode-ipc-3e931bce-e45e-426c-a148-ee604939911e.sock', '__TELEMETRY_CLIENT_ID': '34b29427-cc66-45ae-ad56-875abb609874', 'VSCODE_L10N_BUNDLE_LOCATION': '', 'PYTHONUNBUFFERED': '1', 'CONDA_ROOT': '/home/ubuntu/workspace/llm_model_hub/miniconda3', 'PYTHONIOENCODING': 'utf-8', 'REACT_APP_CALCULATOR': 'https://aws-gpu-memory-caculator.streamlit.app/', 'REACT_APP_API_KEY': 'f1e16e1e6214d7c44d078b1f0607b2388f29d729', 'CONDA_PREFIX_1': '/home/ubuntu/workspace/llm_model_hub/miniconda3', 'REACT_APP_API_ENDPOINT': 'http://ec2-3-93-192-33.compute-1.amazonaws.com:443/v1', 'PYDEVD_IPYTHON_COMPATIBLE_DEBUGGING': '1', 'PYTHON_FROZEN_MODULES': 'on', 'PYDEVD_USE_FRAME_EVAL': 'NO', 'TERM': 'xterm-color', 'CLICOLOR': '1', 'FORCE_COLOR': '1', 'CLICOLOR_FORCE': '1', 'PAGER': 'cat', 'GIT_PAGER': 'cat', 'MPLBACKEND': 'module://matplotlib_inline.backend_inline', 'AK': '', 'SK': '', 'profile': '', 'region': 'us-east-1', 'role': 'arn:aws:iam::434444145045:role/sagemaker-modelhub', 'db_host': '127.0.0.1', 'db_name': 'llm', 'db_user': 'llmdata', 'db_password': 'llmdata', 'api_keys': 'f1e16e1e6214d7c44d078b1f0607b2388f29d729', 'HUGGING_FACE_HUB_TOKEN': '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', 'WANDB_API_KEY': '', 'WANDB_BASE_URL': '', 'SWANLAB_API_KEY': 'q9DzTanu9McPqslOnqg6D', 'vllm_image': '434444145045.dkr.ecr.us-east-1.amazonaws.com/sagemaker_endpoint/vllm:v0.7.2', 'model_artifact': 's3://sagemaker-us-east-1-434444145045/sagemaker_endpoint/vllm/model.tar.gz', 'training_image': '434444145045.dkr.ecr.us-east-1.amazonaws.com/sagemaker/llamafactory:0.9.2.dev0'})\n"
     ]
    }
   ],
   "source": [
    "# !pip install boto3 sagemaker transformers\n",
    "import re\n",
    "import json\n",
    "import os,dotenv\n",
    "import boto3\n",
    "import sagemaker\n",
    "from sagemaker import Model\n",
    "\n",
    "\n",
    "dotenv.load_dotenv()\n",
    "print(os.environ)\n",
    "\n",
    "boto_sess = boto3.Session(\n",
    "    region_name='us-east-1'\n",
    ")\n",
    "\n",
    "sess = sagemaker.session.Session(boto_session=boto_sess)\n",
    "# role = sagemaker.get_execution_role()\n",
    "role = os.environ.get('role')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Prepare model file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Option 2: deploy vllm by model_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model_tar/\n",
      "model_tar/env\n",
      "model_tar/s5cmd\n"
     ]
    }
   ],
   "source": [
    "!tar czvf model.tar.gz model_tar/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "S3 Code or Model tar ball uploaded to --- > s3://sagemaker-us-east-1-434444145045/sagemaker_endpoint/sglang//model.tar.gz\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "s3_code_prefix = f\"sagemaker_endpoint/sglang/\"\n",
    "bucket = sess.default_bucket() \n",
    "code_artifact = sess.upload_data(\"model.tar.gz\", bucket, s3_code_prefix)\n",
    "print(f\"S3 Code or Model tar ball uploaded to --- > {code_artifact}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### 3.3 Deploy model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# CONTAINER='434444145045.dkr.ecr.us-east-1.amazonaws.com/sagemaker_endpoint/vllm:v0.5.5'\n",
    "# model = Model(\n",
    "#     name=sagemaker.utils.name_from_base(\"sagemaker-vllm\")+\"_model\",\n",
    "#     model_data=code_artifact,\n",
    "#     image_uri=CONTAINER,\n",
    "#     role=role,\n",
    "#     sagemaker_session=sess,\n",
    "# )\n",
    "\n",
    "# # 部署模型到endpoint\n",
    "# endpoint_name = sagemaker.utils.name_from_base(\"sagemaker-vllm\")+\"_endpoint\"\n",
    "# print(f\"endpoint_name: {endpoint_name}\")\n",
    "# predictor = model.deploy(\n",
    "#     initial_instance_count=1,\n",
    "#     instance_type='ml.g5.2xlarge',\n",
    "#     endpoint_name=endpoint_name,\n",
    "# )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### test deployment from s3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "endpoint_name: sagemaker-sglang-2025-02-19-01-35-37-565-endpoint\n"
     ]
    }
   ],
   "source": [
    "CONTAINER='434444145045.dkr.ecr.us-east-1.amazonaws.com/sagemaker_endpoint/sglang:v0.4.3.post2-cu124'\n",
    "model_path = \"s3://sagemaker-us-east-1-434444145045/Qwen2-5-3B-Instruct/032650faedac452e86f95f3f3b004342/finetuned_model/\"\n",
    "model_id = 'Qwen/Qwen2-1.5B-Instruct'\n",
    "env={\n",
    "    # \"HF_MODEL_ID\": model_id,\n",
    "    \"S3_MODEL_PATH\":model_path,\n",
    "}\n",
    "model = Model(\n",
    "    name=sagemaker.utils.name_from_base(\"sagemaker-sglang\")+\"-model\",\n",
    "    model_data=code_artifact,\n",
    "    image_uri=CONTAINER,\n",
    "    role=role,\n",
    "    sagemaker_session=sess,\n",
    "    env=env,\n",
    ")\n",
    "\n",
    "# 部署模型到endpoint\n",
    "endpoint_name = sagemaker.utils.name_from_base(\"sagemaker-sglang\")+\"-endpoint\"\n",
    "print(f\"endpoint_name: {endpoint_name}\")\n",
    "predictor = model.deploy(\n",
    "    initial_instance_count=1,\n",
    "    instance_type='ml.g5.2xlarge',\n",
    "    endpoint_name=endpoint_name,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Test\n",
    "\n",
    "you can invoke your model with SageMaker SDK"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1 Message api non-stream mode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I am a large language model created by Alibaba Cloud. I am called Qwen.\n"
     ]
    }
   ],
   "source": [
    "runtime = boto3.client('runtime.sagemaker',region_name='us-east-1')\n",
    "endpoint_name = \"sagemaker-sglang-2025-02-18-14-55-17-330-endpoint\"\n",
    "payload = {\n",
    "    \"messages\": [\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": \"who are you\"\n",
    "    }\n",
    "    ],\n",
    "    \"model\":\"qwen\",\n",
    "    \"max_tokens\": 1024,\n",
    "    \"stream\": False\n",
    "}\n",
    "response = runtime.invoke_endpoint(\n",
    "    EndpointName=endpoint_name,\n",
    "    ContentType='application/json',\n",
    "    Body=json.dumps(payload)\n",
    ")\n",
    "\n",
    "print(json.loads(response['Body'].read())[\"choices\"][0][\"message\"][\"content\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### 4.2 Message api stream mode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Here is an implementation of the quick sort algorithm in Python:\n",
      "```python\n",
      "def quicksort(arr):\n",
      "    if len(arr) <= 1:\n",
      "        return arr\n",
      "    pivot = arr[len(arr) // 2]\n",
      "    left = [x for x in arr if x < pivot]\n",
      "    middle = [x for x in arr if x == pivot]\n",
      "    right = [x for x in arr if x > pivot]\n",
      "    return quicksort(left) + middle + quicksort(right)\n",
      "```\n",
      "\n",
      "This function takes an array as input and returns a sorted version of it. It works by first checking if the array is already sorted, in which case it simply returns the array. Otherwise, it chooses a pivot element (here, the middle element of the array) and partitions the array into three parts: elements that are less than the pivot, elements that are equal to the pivot, and elements that are greater than the pivot. It then recursively sorts the left and right partitions and concatenates them with the middle partition to produce the final sorted array."
     ]
    }
   ],
   "source": [
    "payload = {\n",
    "    \"messages\": [\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": \"Write a quick sort in python\"\n",
    "    }\n",
    "    ],\n",
    "    \"model\":\"qwen\",\n",
    "    \"max_tokens\": 4096,\n",
    "    \"stream\": True\n",
    "}\n",
    "\n",
    "response = runtime.invoke_endpoint_with_response_stream(\n",
    "    EndpointName=endpoint_name,\n",
    "    ContentType='application/json',\n",
    "    Body=json.dumps(payload)\n",
    ")\n",
    "\n",
    "buffer = \"\"\n",
    "for t in response['Body']:\n",
    "    buffer += t[\"PayloadPart\"][\"Bytes\"].decode()\n",
    "    last_idx = 0\n",
    "    for match in re.finditer(r'^data:\\s*(.+?)(\\n\\n)', buffer):\n",
    "        try:\n",
    "            data = json.loads(match.group(1).strip())\n",
    "            last_idx = match.span()[1]\n",
    "            print(data[\"choices\"][0][\"delta\"][\"content\"], end=\"\")\n",
    "        except (json.JSONDecodeError, KeyError, IndexError) as e:\n",
    "            pass\n",
    "    buffer = buffer[last_idx:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3 Completion api non-stream mode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"deepseek-ai/deepseek-coder-6.7b-instruct\", trust_remote_code=True)\n",
    "messages=[\n",
    "    { 'role': 'user', 'content': \"write a quick sort algorithm in python.\"}\n",
    "]\n",
    "prompt = tokenizer.apply_chat_template(messages, add_generation_prompt=True, tokenize=False)\n",
    "\n",
    "payload = {\n",
    "    \"model\": \"deepseek-ai/deepseek-coder-1.3b-instruct\",\n",
    "    \"prompt\": prompt,\n",
    "    \"max_tokens\": 1024,\n",
    "    \"stream\": False\n",
    "}\n",
    "\n",
    "response = runtime.invoke_endpoint(\n",
    "    EndpointName=endpoint_name,\n",
    "    ContentType='application/json',\n",
    "    Body=json.dumps(payload)\n",
    ")\n",
    "\n",
    "print(json.loads(response['Body'].read())[\"choices\"][0][\"text\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.4 Completion api stream mode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "payload = {\n",
    "    \"model\": \"deepseek-ai/deepseek-coder-1.3b-instruct\",\n",
    "    \"prompt\": prompt,\n",
    "    \"max_tokens\": 1024,\n",
    "    \"stream\": True\n",
    "}\n",
    "\n",
    "response = runtime.invoke_endpoint_with_response_stream(\n",
    "    EndpointName=endpoint_name,\n",
    "    ContentType='application/json',\n",
    "    Body=json.dumps(payload)\n",
    ")\n",
    "\n",
    "buffer = \"\"\n",
    "for t in response['Body']:\n",
    "    buffer += t[\"PayloadPart\"][\"Bytes\"].decode()\n",
    "    last_idx = 0\n",
    "    for match in re.finditer(r'^data:\\s*(.+?)(\\n\\n)', buffer):\n",
    "        try:\n",
    "            data = json.loads(match.group(1).strip())\n",
    "            last_idx = match.end()\n",
    "            # print(data)\n",
    "            print(data[\"choices\"][0][\"text\"], end=\"\")\n",
    "        except (json.JSONDecodeError, KeyError, IndexError) as e:\n",
    "            pass\n",
    "    buffer = buffer[last_idx:]\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py311",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
