{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "# SageMaker Example"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Build the container\n",
    "\n",
    "demo codes are in `app/`\n",
    "build and push the docker with following commands:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "set -e\n",
      "\n",
      "# This script shows how to build the Docker image and push it to ECR to be ready for use\n",
      "# by SageMaker.\n",
      "\n",
      "# The argument to this script is the region name. \n",
      "# 尝试使用 IMDSv2 获取 token\n",
      "TOKEN=$(curl -X PUT \"http://169.254.169.254/latest/api/token\" -H \"X-aws-ec2-metadata-token-ttl-seconds: 21600\")\n",
      "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
      "                                 Dload  Upload   Total   Spent    Left  Speed\n",
      "100    56  100    56    0     0  45197      0 --:--:-- --:--:-- --:--:-- 56000\n",
      "\n",
      "# Get the current region and write it to the backend .env file\n",
      "region=$(curl -H \"X-aws-ec2-metadata-token: $TOKEN\" -s http://169.254.169.254/latest/meta-data/placement/region)\n",
      "# region=$(aws configure get region)\n",
      "suffix=\"com\"\n",
      "\n",
      "if [[ $region =~ ^cn ]]; then\n",
      "    suffix=\"com.cn\"\n",
      "fi\n",
      "\n",
      "# Get the account number associated with the current IAM credentials\n",
      "account=$(aws sts  get-caller-identity --query Account --output text)\n",
      "\n",
      "SGL_VERSION=latest\n",
      "inference_image=sagemaker_endpoint/sglang\n",
      "inference_fullname=${account}.dkr.ecr.${region}.amazonaws.${suffix}/${inference_image}:${SGL_VERSION}\n",
      "\n",
      "# If the repository doesn't exist in ECR, create it.\n",
      "aws  ecr describe-repositories --repository-names \"${inference_image}\" --region ${region} || aws ecr create-repository --repository-name \"${inference_image}\" --region ${region}\n",
      "{\n",
      "    \"repositories\": [\n",
      "        {\n",
      "            \"repositoryArn\": \"arn:aws:ecr:us-east-1:434444145045:repository/sagemaker_endpoint/sglang\",\n",
      "            \"registryId\": \"434444145045\",\n",
      "            \"repositoryName\": \"sagemaker_endpoint/sglang\",\n",
      "            \"repositoryUri\": \"434444145045.dkr.ecr.us-east-1.amazonaws.com/sagemaker_endpoint/sglang\",\n",
      "            \"createdAt\": 1739108011.378,\n",
      "            \"imageTagMutability\": \"MUTABLE\",\n",
      "            \"imageScanningConfiguration\": {\n",
      "                \"scanOnPush\": false\n",
      "            },\n",
      "            \"encryptionConfiguration\": {\n",
      "                \"encryptionType\": \"AES256\"\n",
      "            }\n",
      "        }\n",
      "    ]\n",
      "}\n",
      "\n",
      "if [ $? -ne 0 ]\n",
      "then\n",
      "    aws  ecr create-repository --repository-name \"${inference_image}\" --region ${region}\n",
      "fi\n",
      "\n",
      "# Get the login command from ECR and execute it directly\n",
      "aws  ecr get-login-password --region $region | docker login --username AWS --password-stdin $account.dkr.ecr.$region.amazonaws.${suffix}\n",
      "WARNING! Your password will be stored unencrypted in /home/ubuntu/.docker/config.json.\n",
      "Configure a credential helper to remove this warning. See\n",
      "https://docs.docker.com/engine/reference/commandline/login/#credentials-store\n",
      "\n",
      "Login Succeeded\n",
      "\n",
      "aws ecr set-repository-policy \\\n",
      "    --repository-name \"${inference_image}\" \\\n",
      "    --policy-text \"file://ecr-policy.json\" \\\n",
      "    --region ${region}\n",
      "{\n",
      "    \"registryId\": \"434444145045\",\n",
      "    \"repositoryName\": \"sagemaker_endpoint/sglang\",\n",
      "    \"policyText\": \"{\\n  \\\"Version\\\" : \\\"2008-10-17\\\",\\n  \\\"Statement\\\" : [ {\\n    \\\"Sid\\\" : \\\"new statement\\\",\\n    \\\"Effect\\\" : \\\"Allow\\\",\\n    \\\"Principal\\\" : \\\"*\\\",\\n    \\\"Action\\\" : [ \\\"ecr: CompleteLayerUpload\\\", \\\"ecr: InitiateLayerUpload\\\", \\\"ecr: ListImages\\\", \\\"ecr:BatchCheckLayerAvailability\\\", \\\"ecr:BatchGetImage\\\", \\\"ecr:DescribeImages\\\", \\\"ecr:DescribeRepositories\\\", \\\"ecr:GetDownloadUrlForLayer\\\" ]\\n  } ]\\n}\"\n",
      "}\n",
      "\n",
      "# Build the docker image locally with the image name and then push it to ECR\n",
      "# with the full name.\n",
      "\n",
      "docker build  --build-arg SGL_VERSION=${SGL_VERSION} -t ${inference_image}:${SGL_VERSION}  -f Dockerfile.sglang . \n",
      "\u001b[1A\u001b[1B\u001b[0G\u001b[?25l[+] Building 0.0s (0/2)                                          docker:default\n",
      "\u001b[?25h\u001b[1A\u001b[0G\u001b[?25l[+] Building 0.1s (10/11)                                        docker:default\n",
      "\u001b[34m => [internal] load .dockerignore                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2B                                            0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load build definition from Dockerfile.sglang                0.0s\n",
      "\u001b[0m\u001b[34m => => transferring dockerfile: 953B                                       0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load metadata for docker.io/lmsysorg/sglang:latest          0.1s\n",
      "\u001b[0m\u001b[34m => [1/6] FROM docker.io/lmsysorg/sglang:latest@sha256:c693c32445ea938e11  0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load build context                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 103B                                          0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [2/6] WORKDIR /app                                              0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [3/6] COPY app_sglang/ /app                                     0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [4/6] COPY requirements.txt /app                                0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [5/6] RUN pip install -r requirements.txt                       0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [6/6] RUN export PYTHON_SITEPACKAGES=`python3 -c \"import sglan  0.0s\n",
      "\u001b[0m => exporting to image                                                     0.0s\n",
      "\u001b[34m => => exporting layers                                                    0.0s\n",
      "\u001b[0m\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 0.1s (11/11) FINISHED                               docker:default\n",
      "\u001b[34m => [internal] load .dockerignore                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2B                                            0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load build definition from Dockerfile.sglang                0.0s\n",
      "\u001b[0m\u001b[34m => => transferring dockerfile: 953B                                       0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load metadata for docker.io/lmsysorg/sglang:latest          0.1s\n",
      "\u001b[0m\u001b[34m => [1/6] FROM docker.io/lmsysorg/sglang:latest@sha256:c693c32445ea938e11  0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load build context                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 103B                                          0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [2/6] WORKDIR /app                                              0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [3/6] COPY app_sglang/ /app                                     0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [4/6] COPY requirements.txt /app                                0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [5/6] RUN pip install -r requirements.txt                       0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [6/6] RUN export PYTHON_SITEPACKAGES=`python3 -c \"import sglan  0.0s\n",
      "\u001b[0m\u001b[34m => exporting to image                                                     0.0s\n",
      "\u001b[0m\u001b[34m => => exporting layers                                                    0.0s\n",
      "\u001b[0m\u001b[34m => => writing image sha256:0b487ad1e21e0f0663340ac26742a8d2601830a17f9db  0.0s\n",
      "\u001b[0m\u001b[34m => => naming to docker.io/sagemaker_endpoint/sglang:latest                0.0s\n",
      "\u001b[0m\u001b[?25h\n",
      "docker tag ${inference_image}:${SGL_VERSION} ${inference_fullname}\n",
      "\n",
      "docker push ${inference_fullname}\n",
      "The push refers to repository [434444145045.dkr.ecr.us-east-1.amazonaws.com/sagemaker_endpoint/sglang]\n",
      "\n",
      "\u001b[1B44721b19: Preparing \n",
      "\u001b[1B012919ce: Preparing \n",
      "\u001b[1Bbad29619: Preparing \n",
      "\u001b[1B53b18379: Preparing \n",
      "\u001b[1B266298fb: Preparing \n",
      "\u001b[1B9a3b8aa7: Preparing \n",
      "\u001b[1Bc87fedfc: Preparing \n",
      "\u001b[1B7495d60d: Preparing \n",
      "\u001b[1B9895f7cd: Preparing \n",
      "\u001b[1B8bbccdd1: Preparing \n",
      "\u001b[1B73680ef3: Preparing \n",
      "\u001b[1B14258878: Preparing \n",
      "\u001b[1B9ed2f631: Preparing \n",
      "\u001b[1Bda6ba3ae: Preparing \n",
      "\u001b[1B70b9a284: Preparing \n",
      "\u001b[1B88486056: Preparing \n",
      "\u001b[1B7c97ed4d: Preparing \n",
      "\u001b[1B3c3b8514: Preparing \n",
      "\u001b[1Bb7e2d072: Preparing \n",
      "\u001b[1B8f6fcd6a: Preparing \n",
      "\u001b[1Bded77c0c: Layer already exists \u001b[16A\u001b[2K\u001b[15A\u001b[2K\u001b[11A\u001b[2K\u001b[5A\u001b[2K\u001b[2A\u001b[2Klatest: digest: sha256:273074cde17b788ebc0db084d0f7eb256ad01f6cd5417d125eeabc02b9a1e44b size: 4721\n",
      "# 删除 .env 文件中的 sglang_image= 这一行\n",
      "sed -i '/^sglang_image=/d' /home/ubuntu/llm_model_hub/backend/.env\n",
      "sed: can't read /home/ubuntu/llm_model_hub/backend/.env: No such file or directory\n"
     ]
    }
   ],
   "source": [
    "!bash build_and_push_sglang.sh"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Deploy on SageMaker\n",
    "\n",
    "define the model and deploy on SageMaker\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### 3.1 Init SageMaker session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/workspace/llm_model_hub/miniconda3/envs/py311/lib/python3.11/site-packages/pydantic/_internal/_fields.py:192: UserWarning: Field name \"json\" in \"MonitoringDatasetFormat\" shadows an attribute in parent \"Base\"\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">[03/01/25 09:02:28] </span><span style=\"color: #0069ff; text-decoration-color: #0069ff; font-weight: bold\">INFO    </span> Found credentials from IAM Role: admin_role_for_workshop           <a href=\"file:///home/ubuntu/workspace/llm_model_hub/miniconda3/envs/py311/lib/python3.11/site-packages/botocore/credentials.py\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">credentials.py</span></a><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">:</span><a href=\"file:///home/ubuntu/workspace/llm_model_hub/miniconda3/envs/py311/lib/python3.11/site-packages/botocore/credentials.py#1132\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1132</span></a>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[2;36m[03/01/25 09:02:28]\u001b[0m\u001b[2;36m \u001b[0m\u001b[1;38;2;0;105;255mINFO    \u001b[0m Found credentials from IAM Role: admin_role_for_workshop           \u001b]8;id=17272;file:///home/ubuntu/workspace/llm_model_hub/miniconda3/envs/py311/lib/python3.11/site-packages/botocore/credentials.py\u001b\\\u001b[2mcredentials.py\u001b[0m\u001b]8;;\u001b\\\u001b[2m:\u001b[0m\u001b]8;id=248366;file:///home/ubuntu/workspace/llm_model_hub/miniconda3/envs/py311/lib/python3.11/site-packages/botocore/credentials.py#1132\u001b\\\u001b[2m1132\u001b[0m\u001b]8;;\u001b\\\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sagemaker.config INFO - Not applying SDK defaults from location: /etc/xdg/sagemaker/config.yaml\n",
      "sagemaker.config INFO - Not applying SDK defaults from location: /home/ubuntu/.config/sagemaker/config.yaml\n",
      "environ({'USER': 'ubuntu', 'SSH_CLIENT': '52.94.133.139 4026 22', 'XDG_SESSION_TYPE': 'tty', 'SHLVL': '2', 'HOME': '/home/ubuntu', 'SSL_CERT_FILE': '/usr/lib/ssl/cert.pem', 'DBUS_SESSION_BUS_ADDRESS': 'unix:path=/run/user/1000/bus', 'LOGNAME': 'ubuntu', '_': '/home/ubuntu/workspace/llm_model_hub/miniconda3/envs/py311/bin/python', 'XDG_SESSION_CLASS': 'user', 'XDG_SESSION_ID': '34414', 'VSCODE_CLI_REQUIRE_TOKEN': '3ae02bac-b0cf-4129-8cc1-92a260fb9929', 'PATH': '/home/ubuntu/workspace/llm_model_hub/miniconda3/envs/py311/bin:/home/ubuntu/.vscode-server/cli/servers/Stable-e54c774e0add60467559eb0d1e229c6452cf8447/server/bin/remote-cli:/home/ubuntu/.local/bin:/home/ubuntu/workspace/llm_model_hub/miniconda3/envs/py311/bin:/home/ubuntu/workspace/llm_model_hub/miniconda3/condabin:/home/ubuntu/.local/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/usr/games:/usr/local/games:/snap/bin', 'VSCODE_AGENT_FOLDER': '/home/ubuntu/.vscode-server', 'XDG_RUNTIME_DIR': '/run/user/1000', 'SSL_CERT_DIR': '/usr/lib/ssl/certs', 'LANG': 'C.UTF-8', 'SHELL': '/bin/bash', 'PWD': '/home/ubuntu', 'SSH_CONNECTION': '52.94.133.139 4026 172.31.38.107 22', 'XDG_DATA_DIRS': '/usr/local/share:/usr/share:/var/lib/snapd/desktop', 'VSCODE_CWD': '/home/ubuntu', 'VSCODE_NLS_CONFIG': '{\"userLocale\":\"en\",\"osLocale\":\"en\",\"resolvedLanguage\":\"en\",\"defaultMessagesFile\":\"/home/ubuntu/.vscode-server/cli/servers/Stable-e54c774e0add60467559eb0d1e229c6452cf8447/server/out/nls.messages.json\",\"locale\":\"en\",\"availableLanguages\":{}}', 'VSCODE_HANDLES_SIGPIPE': 'true', 'CONDA_EXE': '/home/ubuntu/workspace/llm_model_hub/miniconda3/bin/conda', '_CE_M': '', 'CONDA_PREFIX': '/home/ubuntu/workspace/llm_model_hub/miniconda3/envs/py311', 'LS_COLORS': '', 'CONDA_PROMPT_MODIFIER': '(py311) ', 'LESSCLOSE': '/usr/bin/lesspipe %s %s', '_CE_CONDA': '', 'LESSOPEN': '| /usr/bin/lesspipe %s', 'CONDA_SHLVL': '2', 'CONDA_PYTHON_EXE': '/home/ubuntu/workspace/llm_model_hub/miniconda3/bin/python', 'CONDA_DEFAULT_ENV': 'py311', 'VSCODE_ESM_ENTRYPOINT': 'vs/workbench/api/node/extensionHostProcess', 'VSCODE_HANDLES_UNCAUGHT_ERRORS': 'true', 'BROWSER': '/home/ubuntu/.vscode-server/cli/servers/Stable-e54c774e0add60467559eb0d1e229c6452cf8447/server/bin/helpers/browser.sh', 'ELECTRON_RUN_AS_NODE': '1', 'VSCODE_IPC_HOOK_CLI': '/run/user/1000/vscode-ipc-f1b7a5a9-8226-4a39-aeb2-52cfeda51052.sock', '__TELEMETRY_CLIENT_ID': '34b29427-cc66-45ae-ad56-875abb609874', 'VSCODE_L10N_BUNDLE_LOCATION': '', 'PYTHONUNBUFFERED': '1', 'CONDA_ROOT': '/home/ubuntu/workspace/llm_model_hub/miniconda3', 'PYTHONIOENCODING': 'utf-8', 'REACT_APP_CALCULATOR': 'https://aws-gpu-memory-caculator.streamlit.app/', 'REACT_APP_API_KEY': 'f1e16e1e6214d7c44d078b1f0607b2388f29d729', 'CONDA_PREFIX_1': '/home/ubuntu/workspace/llm_model_hub/miniconda3', 'REACT_APP_API_ENDPOINT': 'http://ec2-3-93-192-33.compute-1.amazonaws.com:443/v1', 'PYDEVD_IPYTHON_COMPATIBLE_DEBUGGING': '1', 'PYTHON_FROZEN_MODULES': 'on', 'PYDEVD_USE_FRAME_EVAL': 'NO', 'TERM': 'xterm-color', 'CLICOLOR': '1', 'FORCE_COLOR': '1', 'CLICOLOR_FORCE': '1', 'PAGER': 'cat', 'GIT_PAGER': 'cat', 'MPLBACKEND': 'module://matplotlib_inline.backend_inline', 'AK': '', 'SK': '', 'profile': '', 'region': 'us-east-1', 'role': 'arn:aws:iam::434444145045:role/sagemaker-modelhub', 'db_host': '127.0.0.1', 'db_name': 'llm', 'db_user': 'llmdata', 'db_password': 'llmdata', 'api_keys': 'f1e16e1e6214d7c44d078b1f0607b2388f29d729', 'HUGGING_FACE_HUB_TOKEN': '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', 'WANDB_API_KEY': '', 'WANDB_BASE_URL': '', 'SWANLAB_API_KEY': 'q9DzTanu9McPqslOnqg6D', 'vllm_image': '434444145045.dkr.ecr.us-east-1.amazonaws.com/sagemaker_endpoint/vllm:v0.7.2', 'model_artifact': 's3://sagemaker-us-east-1-434444145045/sagemaker_endpoint/vllm/model.tar.gz', 'training_image': '434444145045.dkr.ecr.us-east-1.amazonaws.com/sagemaker/llamafactory:0.9.2.dev0', 'sglang_image': '434444145045.dkr.ecr.us-east-1.amazonaws.com/sagemaker_endpoint/sglang:v0.4.3.post2-cu124'})\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">                    </span><span style=\"color: #0069ff; text-decoration-color: #0069ff; font-weight: bold\">INFO    </span> Found credentials from IAM Role: admin_role_for_workshop           <a href=\"file:///home/ubuntu/workspace/llm_model_hub/miniconda3/envs/py311/lib/python3.11/site-packages/botocore/credentials.py\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">credentials.py</span></a><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">:</span><a href=\"file:///home/ubuntu/workspace/llm_model_hub/miniconda3/envs/py311/lib/python3.11/site-packages/botocore/credentials.py#1132\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1132</span></a>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[2;36m                   \u001b[0m\u001b[2;36m \u001b[0m\u001b[1;38;2;0;105;255mINFO    \u001b[0m Found credentials from IAM Role: admin_role_for_workshop           \u001b]8;id=560895;file:///home/ubuntu/workspace/llm_model_hub/miniconda3/envs/py311/lib/python3.11/site-packages/botocore/credentials.py\u001b\\\u001b[2mcredentials.py\u001b[0m\u001b]8;;\u001b\\\u001b[2m:\u001b[0m\u001b]8;id=189624;file:///home/ubuntu/workspace/llm_model_hub/miniconda3/envs/py311/lib/python3.11/site-packages/botocore/credentials.py#1132\u001b\\\u001b[2m1132\u001b[0m\u001b]8;;\u001b\\\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# !pip install boto3 sagemaker transformers\n",
    "import re\n",
    "import json\n",
    "import os,dotenv\n",
    "import boto3\n",
    "import sagemaker\n",
    "from sagemaker import Model\n",
    "\n",
    "\n",
    "dotenv.load_dotenv()\n",
    "print(os.environ)\n",
    "\n",
    "boto_sess = boto3.Session(\n",
    "    region_name='us-east-1'\n",
    ")\n",
    "\n",
    "sess = sagemaker.session.Session(boto_session=boto_sess)\n",
    "# role = sagemaker.get_execution_role()\n",
    "role = os.environ.get('role')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Prepare model file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Option 2: deploy vllm by model_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model_tar/\n",
      "model_tar/env\n",
      "model_tar/s5cmd\n"
     ]
    }
   ],
   "source": [
    "!tar czvf model.tar.gz model_tar/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "S3 Code or Model tar ball uploaded to --- > s3://sagemaker-us-east-1-434444145045/sagemaker_endpoint/sglang//model.tar.gz\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "s3_code_prefix = f\"sagemaker_endpoint/sglang/\"\n",
    "bucket = sess.default_bucket() \n",
    "code_artifact = sess.upload_data(\"model.tar.gz\", bucket, s3_code_prefix)\n",
    "print(f\"S3 Code or Model tar ball uploaded to --- > {code_artifact}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### 3.3 Deploy model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# CONTAINER='434444145045.dkr.ecr.us-east-1.amazonaws.com/sagemaker_endpoint/vllm:v0.7.2'\n",
    "\n",
    "# env={\n",
    "#     \"HF_MODEL_ID\": model_name,\n",
    "#     \"DTYPE\": dtype,\n",
    "#     \"LIMIT_MM_PER_PROMPT\":extra_params.get('limit_mm_per_prompt',''),\n",
    "#     \"S3_MODEL_PATH\":model_path,\n",
    "#     \"VLLM_ALLOW_LONG_MAX_MODEL_LEN\":\"1\",\n",
    "#     \"HF_TOKEN\":os.environ.get('HUGGING_FACE_HUB_TOKEN'),\n",
    "#     \"MAX_MODEL_LEN\":extra_params.get('max_model_len', \"12288\"), \n",
    "#     \"ENABLE_PREFIX_CACHING\": \"1\" if extra_params.get('enable_prefix_caching') else \"0\",\n",
    "#     \"TENSOR_PARALLEL_SIZE\": extra_params.get('tensor_parallel_size',str(get_auto_tensor_parallel_size(instance_type))),\n",
    "#     \"MAX_NUM_SEQS\": extra_params.get('max_num_seqs','256'),\n",
    "#     \"ENFORCE_EAGER\": \"1\" if extra_params.get('enforce_eager') else \"0\",\n",
    "\n",
    "#         }\n",
    "\n",
    "# model = Model(\n",
    "#     name=sagemaker.utils.name_from_base(\"sagemaker-vllm\")+\"_model\",\n",
    "#     model_data=code_artifact,\n",
    "#     image_uri=CONTAINER,\n",
    "#     role=role,\n",
    "#     sagemaker_session=sess,\n",
    "#     env=env,\n",
    "    \n",
    "# )\n",
    "\n",
    "\n",
    "\n",
    "# # 部署模型到endpoint\n",
    "# endpoint_name = sagemaker.utils.name_from_base(\"sagemaker-vllm\")+\"_endpoint\"\n",
    "# print(f\"endpoint_name: {endpoint_name}\")\n",
    "# predictor = model.deploy(\n",
    "#     initial_instance_count=1,\n",
    "#     instance_type='ml.g5.2xlarge',\n",
    "#     endpoint_type = EndpointType.INFERENCE_COMPONENT_BASED,\n",
    "#     endpoint_name=endpoint_name,\n",
    "# )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### test deployment from s3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "endpoint_name: sagemaker-sglang-2025-03-01-09-03-13-505-endpoint\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">[03/01/25 09:03:13] </span><span style=\"color: #0069ff; text-decoration-color: #0069ff; font-weight: bold\">INFO    </span> Creating endpoint-config with name                                     <a href=\"file:///home/ubuntu/workspace/llm_model_hub/miniconda3/envs/py311/lib/python3.11/site-packages/sagemaker/session.py\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">session.py</span></a><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">:</span><a href=\"file:///home/ubuntu/workspace/llm_model_hub/miniconda3/envs/py311/lib/python3.11/site-packages/sagemaker/session.py#5889\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">5889</span></a>\n",
       "<span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">                    </span>         sagemaker-sglang-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2025</span>-03-01-09-03-13-505-endpoint                      <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">               </span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[2;36m[03/01/25 09:03:13]\u001b[0m\u001b[2;36m \u001b[0m\u001b[1;38;2;0;105;255mINFO    \u001b[0m Creating endpoint-config with name                                     \u001b]8;id=435273;file:///home/ubuntu/workspace/llm_model_hub/miniconda3/envs/py311/lib/python3.11/site-packages/sagemaker/session.py\u001b\\\u001b[2msession.py\u001b[0m\u001b]8;;\u001b\\\u001b[2m:\u001b[0m\u001b]8;id=997021;file:///home/ubuntu/workspace/llm_model_hub/miniconda3/envs/py311/lib/python3.11/site-packages/sagemaker/session.py#5889\u001b\\\u001b[2m5889\u001b[0m\u001b]8;;\u001b\\\n",
       "\u001b[2;36m                    \u001b[0m         sagemaker-sglang-\u001b[1;36m2025\u001b[0m-03-01-09-03-13-505-endpoint                      \u001b[2m               \u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">                    </span><span style=\"color: #0069ff; text-decoration-color: #0069ff; font-weight: bold\">INFO    </span> Creating endpoint with name                                            <a href=\"file:///home/ubuntu/workspace/llm_model_hub/miniconda3/envs/py311/lib/python3.11/site-packages/sagemaker/session.py\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">session.py</span></a><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">:</span><a href=\"file:///home/ubuntu/workspace/llm_model_hub/miniconda3/envs/py311/lib/python3.11/site-packages/sagemaker/session.py#4711\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">4711</span></a>\n",
       "<span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">                    </span>         sagemaker-sglang-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2025</span>-03-01-09-03-13-505-endpoint                      <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">               </span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[2;36m                   \u001b[0m\u001b[2;36m \u001b[0m\u001b[1;38;2;0;105;255mINFO    \u001b[0m Creating endpoint with name                                            \u001b]8;id=942025;file:///home/ubuntu/workspace/llm_model_hub/miniconda3/envs/py311/lib/python3.11/site-packages/sagemaker/session.py\u001b\\\u001b[2msession.py\u001b[0m\u001b]8;;\u001b\\\u001b[2m:\u001b[0m\u001b]8;id=634910;file:///home/ubuntu/workspace/llm_model_hub/miniconda3/envs/py311/lib/python3.11/site-packages/sagemaker/session.py#4711\u001b\\\u001b[2m4711\u001b[0m\u001b]8;;\u001b\\\n",
       "\u001b[2;36m                    \u001b[0m         sagemaker-sglang-\u001b[1;36m2025\u001b[0m-03-01-09-03-13-505-endpoint                      \u001b[2m               \u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-"
     ]
    }
   ],
   "source": [
    "from sagemaker.enums import EndpointType\n",
    "from sagemaker.compute_resource_requirements.resource_requirements import ResourceRequirements\n",
    "from sagemaker import Predictor\n",
    "from sagemaker import Model\n",
    "\n",
    "resources = ResourceRequirements(\n",
    "    requests = {\n",
    "        # \"num_cpus\": 4,  # Number of CPU cores required:\n",
    "        \"num_accelerators\": 1, # Number of accelerators required\n",
    "        \"memory\": 1024*4,  # Minimum memory required in Mb (required)\n",
    "        \"copies\": 1,\n",
    "    },\n",
    "    limits = {},\n",
    ")\n",
    "\n",
    "CONTAINER='434444145045.dkr.ecr.us-east-1.amazonaws.com/sagemaker_endpoint/sglang:v0.4.3.post2-cu124'\n",
    "model_path = \"s3://sagemaker-us-east-1-434444145045/Qwen2-5-3B-Instruct/032650faedac452e86f95f3f3b004342/finetuned_model/\"\n",
    "model_id = 'Qwen/Qwen2-1.5B-Instruct'\n",
    "env={\n",
    "    \"HF_MODEL_ID\": model_id,\n",
    "    \"S3_MODEL_PATH\":model_path,\n",
    "}\n",
    "\n",
    "model_name = sagemaker.utils.name_from_base(\"sagemaker-sglang\")+\"-model\"\n",
    "\n",
    "model = Model(\n",
    "    name=model_name,\n",
    "    model_data=code_artifact,\n",
    "    image_uri=CONTAINER,\n",
    "    role=role,\n",
    "    sagemaker_session=sess,\n",
    "    resources = resources,\n",
    "    env=env,\n",
    "    predictor_cls = Predictor,\n",
    ")\n",
    "\n",
    "# Create the model in SageMaker\n",
    "# model.create()\n",
    "\n",
    "# 部署模型到endpoint\n",
    "endpoint_name = sagemaker.utils.name_from_base(\"sagemaker-sglang\")+\"-endpoint\"\n",
    "print(f\"endpoint_name: {endpoint_name}\")\n",
    "predictor = model.deploy(\n",
    "    initial_instance_count=1,\n",
    "    instance_type='ml.g5.2xlarge',\n",
    "    endpoint_name=endpoint_name,\n",
    "    resources = resources,\n",
    "    endpoint_type = EndpointType.INFERENCE_COMPONENT_BASED,\n",
    "    model_name=model_name, \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sagemaker\n",
    "from sagemaker import Model, image_uris, serializers, deserializers\n",
    "from sagemaker import get_execution_role\n",
    "from sagemaker.pytorch import PyTorchModel\n",
    "\n",
    "sm_client = boto3.client(service_name=\"sagemaker\")\n",
    "model_name = sagemaker.utils.name_from_base(\"sagemaker-sglang\")+\"-model\"\n",
    "endpoint_name = sagemaker.utils.name_from_base(\"sagemaker-sglang\")+\"-endpoint\"\n",
    "component_name = sagemaker.utils.name_from_base(\"sagemaker-sglang\")+\"-component\"\n",
    "env={\n",
    "    # \"HF_MODEL_ID\": model_id,\n",
    "    \"S3_MODEL_PATH\":model_path,\n",
    "}\n",
    "container_config = {\n",
    "    'Image': CONTAINER,\n",
    "    'ModelDataUrl': code_artifact,\n",
    "    'Environment': env\n",
    "}\n",
    "\n",
    "response = sm_client.create_model(\n",
    "    ModelName=model_name,\n",
    "    ExecutionRoleArn=role,\n",
    "    PrimaryContainer=container_config\n",
    ")\n",
    "\n",
    "print(f\"Model created: {response['ModelArn']}\")\n",
    "\n",
    "sm_client.create_inference_component(\n",
    "    InferenceComponentName=component_name,\n",
    "    EndpointName=endpoint_name,\n",
    "    VariantName=\"AllTraffic\",\n",
    "    Specification={\n",
    "        \"ModelName\": model_name,\n",
    "        \"ComputeResourceRequirements\": {\n",
    "\t\t    \"NumberOfAcceleratorDevicesRequired\": 1, \n",
    "\t\t\t#\"NumberOfCpuCoresRequired\": 2, \n",
    "\t\t\t\"MinMemoryRequiredInMb\": 1024*8\n",
    "\t    }\n",
    "    },\n",
    "    RuntimeConfig={\"CopyCount\": 1},\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Test\n",
    "\n",
    "you can invoke your model with SageMaker SDK"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1 Message api non-stream mode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "ename": "ValidationError",
     "evalue": "An error occurred (ValidationError) when calling the InvokeEndpoint operation: Inference Component Name header is required for endpoints to which you plan to deploy inference components. Please include Inference Component Name header or consider using SageMaker models.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValidationError\u001b[0m                           Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[27], line 14\u001b[0m\n\u001b[1;32m      2\u001b[0m endpoint_name \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msagemaker-sglang-2025-02-24-06-45-27-924-endpoint\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m      3\u001b[0m payload \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m      4\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmessages\u001b[39m\u001b[38;5;124m\"\u001b[39m: [\n\u001b[1;32m      5\u001b[0m     {\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     12\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstream\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m     13\u001b[0m }\n\u001b[0;32m---> 14\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[43mruntime\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minvoke_endpoint\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     15\u001b[0m \u001b[43m    \u001b[49m\u001b[43mEndpointName\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mendpoint_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     16\u001b[0m \u001b[43m    \u001b[49m\u001b[43mContentType\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mapplication/json\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     17\u001b[0m \u001b[43m    \u001b[49m\u001b[43mBody\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mjson\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdumps\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpayload\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     18\u001b[0m \u001b[43m)\u001b[49m\n\u001b[1;32m     20\u001b[0m \u001b[38;5;28mprint\u001b[39m(json\u001b[38;5;241m.\u001b[39mloads(response[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mBody\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mread())[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mchoices\u001b[39m\u001b[38;5;124m\"\u001b[39m][\u001b[38;5;241m0\u001b[39m][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmessage\u001b[39m\u001b[38;5;124m\"\u001b[39m][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcontent\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n",
      "File \u001b[0;32m~/workspace/llm_model_hub/miniconda3/envs/py311/lib/python3.11/site-packages/botocore/client.py:565\u001b[0m, in \u001b[0;36mClientCreator._create_api_method.<locals>._api_call\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    561\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\n\u001b[1;32m    562\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpy_operation_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m() only accepts keyword arguments.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    563\u001b[0m     )\n\u001b[1;32m    564\u001b[0m \u001b[38;5;66;03m# The \"self\" in this scope is referring to the BaseClient.\u001b[39;00m\n\u001b[0;32m--> 565\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_make_api_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43moperation_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/workspace/llm_model_hub/miniconda3/envs/py311/lib/python3.11/site-packages/botocore/client.py:1021\u001b[0m, in \u001b[0;36mBaseClient._make_api_call\u001b[0;34m(self, operation_name, api_params)\u001b[0m\n\u001b[1;32m   1017\u001b[0m     error_code \u001b[38;5;241m=\u001b[39m error_info\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mQueryErrorCode\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m error_info\u001b[38;5;241m.\u001b[39mget(\n\u001b[1;32m   1018\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCode\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1019\u001b[0m     )\n\u001b[1;32m   1020\u001b[0m     error_class \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mexceptions\u001b[38;5;241m.\u001b[39mfrom_code(error_code)\n\u001b[0;32m-> 1021\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m error_class(parsed_response, operation_name)\n\u001b[1;32m   1022\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1023\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m parsed_response\n",
      "\u001b[0;31mValidationError\u001b[0m: An error occurred (ValidationError) when calling the InvokeEndpoint operation: Inference Component Name header is required for endpoints to which you plan to deploy inference components. Please include Inference Component Name header or consider using SageMaker models."
     ]
    }
   ],
   "source": [
    "runtime = boto3.client('runtime.sagemaker',region_name='us-east-1')\n",
    "endpoint_name = \"sagemaker-sglang-2025-02-24-06-45-27-924-endpoint\"\n",
    "payload = {\n",
    "    \"messages\": [\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": \"who are you\"\n",
    "    }\n",
    "    ],\n",
    "    \"model\":\"qwen\",\n",
    "    \"max_tokens\": 1024,\n",
    "    \"stream\": False\n",
    "}\n",
    "response = runtime.invoke_endpoint(\n",
    "    EndpointName=endpoint_name,\n",
    "    ContentType='application/json',\n",
    "    Body=json.dumps(payload)\n",
    ")\n",
    "\n",
    "print(json.loads(response['Body'].read())[\"choices\"][0][\"message\"][\"content\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### 4.2 Message api stream mode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<think>\n",
      "嗯，用户让我写一个Python的快速排序。好的，首先我得回忆一下快速排序的基本原理。快速排序是通过分治法来排序的，基本思想是选一个基准元素，把数组分成比基准小的和比基准大的两个子数组，然后递归地对这两个子数组进行排序。对吧？\n",
      "\n",
      "那首先我得想一下怎么选择基准元素。通常的做法是选第一个元素，比如数组中的第一个元素作为基准。或者可以用中间元素，比如len(arr)//2这样的，不过可能不太好。或者用随机选一个基准，这样分割的时候可能分布更均匀。比如可能用户可能希望用第一个或者中间的元素作为基准，这样代码更简单。\n",
      "\n",
      "那基准的选择方法可能影响排序的效率，但对于普通情况来说，可能第一个元素或者中间的元素就足够了。比如，假设数组是 [5, 2, 9, 1, 5]，选第一个5作为基准的话，分割的时候会把小于等于5的放在左边，大于的放在右边。然后递归处理左边和右边的子数组。这样应该可行。\n",
      "\n",
      "接下来，实现步骤大概是这样的：首先分割数组，然后递归地处理左右子数组。分割的过程需要一个左指针和一个右指针，初始时左指针在0，右指针在最后一个元素的位置。然后交换元素直到左指针不小于右指针的时候，基准就放到了正确的位置。\n",
      "\n",
      "那具体的步骤可能是这样的：\n",
      "\n",
      "1. 如果数组长度小于等于1，返回，因为已经有序。\n",
      "2. 选择基准元素，比如第一个元素。\n",
      "3. 初始化左和右的指针，左从0，右从最后一个元素。\n",
      "4. 交换左指针和右指针移动，直到左指针达到右指针的位置。\n",
      "5. 将基准元素放到正确的位置。\n",
      "6. 递归地对左右子数组排序。\n",
      "\n",
      "现在具体写代码的话，可能需要一个函数，接收数组作为参数。然后处理分割和递归。\n",
      "\n",
      "比如，初始化pivot_index = 0，然后从right指针往左移动，直到left_index < right_index的时候交换。然后把pivot移到pivot_index的位置。然后递归处理left和right子数组。\n",
      "\n",
      "那现在写代码的结构大概是这样的：\n",
      "\n",
      "def quicksort(arr):\n",
      "    if len(arr) <= 1:\n",
      "        return arr\n",
      "    pivot_index = 0\n",
      "    i = 0\n",
      "    j = len(arr) - 1\n",
      "    while i <= j:\n",
      "        while arr[i] < arr[pivot_index]:\n",
      "            i += 1\n",
      "        while arr[j] > arr[pivot_index]:\n",
      "            j -= 1\n",
      "        if i <= j:\n",
      "            arr[i], arr[j] = arr[j], arr[i]\n",
      "            i += 1\n",
      "            j -= 1\n",
      "    # 交换基准到正确的位置\n",
      "    arr[pivot_index], arr[i] = arr[i], arr[pivot_index]\n",
      "    left = quicksort(arr[:i])\n",
      "    right = quicksort(arr[i:])\n",
      "    return left + right\n",
      "\n",
      "这样应该就可以了。不过需要测试一下。比如测试用例是否正确。\n",
      "\n",
      "比如测试用例1：[3,1,4,1,5,9,2,6]，应该排序后是[1,1,2,3,4,5,6,9]。用上面的代码排序后的结果应该是正确的。\n",
      "\n",
      "另一个测试用例是空数组或者单元素数组，比如[1]，应该返回[1]。\n",
      "\n",
      "还有可能需要考虑基准的选择，是否需要随机选？比如在代码中可以添加一个随机选基准的部分，但为了简单，可能先用第一个元素作为基准。\n",
      "\n",
      "那现在把这个代码整理一下，注释清楚，可能的话包括测试用例。\n",
      "\n",
      "另外，注意循环的条件是否正确。原代码中的while i <= j的条件，应该正确，因为当i到j之间的元素已经调整完毕之后，可能i已经超过了j的位置。\n",
      "\n",
      "比如，假设pivot_index是0，那么在交换的过程中，i和j分别从两端向中间移动。当i到j的时候，交换之后，左指针i已经到正确的位置了。然后交换基准到i的位置。然后递归处理左和右子数组。\n",
      "\n",
      "测试一下这个函数是否正确。比如输入[5, 2, 9, 1, 5]，分割的时候基准是5，左右的子数组分别是[2,9,1]和[5,5]。递归处理后得到[1,2,5,5,9]，再合并得到[1,2,5,5,9]。正确。\n",
      "\n",
      "那这应该可以了。那这样写的代码应该就可以了。\n",
      "\n",
      "</think>\n",
      "\n",
      "\n",
      "\n",
      "以下是Python中的快速排序实现，使用分治法和基准元素分割的方式：\n",
      "\n",
      "```python\n",
      "def quicksort(arr):\n",
      "    if len(arr) <= 1:\n",
      "        return arr\n",
      "    pivot_index = 0\n",
      "    left = 0\n",
      "    right = len(arr) - 1\n",
      "    \n",
      "    # 交换基准到正确的位置\n",
      "    arr[pivot_index], arr[right] = arr[right], arr[pivot_index]\n",
      "    \n",
      "    while left <= right:\n",
      "        # 找到大于等于基准的元素位置\n",
      "        while arr[left] < arr[right]:\n",
      "            left += 1\n",
      "        # 找到小于等于基准的元素位置\n",
      "        while arr[right] > arr[left]:\n",
      "            right -= 1\n",
      "        # 交换元素\n",
      "        if left <= right:\n",
      "            arr[left], arr[right] = arr[right], arr[left]\n",
      "            left += 1\n",
      "            right -= 1\n",
      "            \n",
      "    # 将基准放回正确位置\n",
      "    arr[pivot_index], arr[left] = arr[left], arr[pivot_index]\n",
      "    \n",
      "    # 递归排序左边和右边\n",
      "    left_half = quicksort(arr[:left])\n",
      "    right_half = quicksort(arr[left:])\n",
      "    \n",
      "    return left_half + right_half\n",
      "\n",
      "# 示例用法\n",
      "arr = [5, 2, 9, 1, 5, 3, 6, 7]\n",
      "print(quicksort(arr))  # 输出：[1, 2, 3, 5, 5, 6, 7, 9]\n",
      "```\n",
      "\n",
      "**代码说明：**\n",
      "1. **基准选择**：使用最后一个元素作为基准，通过交换将其放置到正确的位置\n",
      "2. **分区过程**：通过两个循环找到大于等于基准和小于基准的元素\n",
      "3. **递归处理**：将数组分为左右两部分分别递归排序\n",
      "4. **时间复杂度**：平均情况下为O(n log n)，最坏情况下为O(n²)，通过合理选择基准可以减少最坏情况\n",
      "\n",
      "该实现避免了Python的列表切片操作，直接操作原数组，效率更高。对于测试用例，输出结果正确，处理空数组和单元素数组也正确。"
     ]
    }
   ],
   "source": [
    "payload = {\n",
    "    \"messages\": [\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": \"Write a quick sort in python\"\n",
    "    }\n",
    "    ],\n",
    "    \"model\":\"custome\",\n",
    "    \"max_tokens\": 4096,\n",
    "    \"stream\": True\n",
    "}\n",
    "\n",
    "response = runtime.invoke_endpoint_with_response_stream(\n",
    "    EndpointName=endpoint_name,\n",
    "    ContentType='application/json',\n",
    "    Body=json.dumps(payload)\n",
    ")\n",
    "\n",
    "buffer = \"\"\n",
    "for t in response['Body']:\n",
    "    buffer += t[\"PayloadPart\"][\"Bytes\"].decode()\n",
    "    last_idx = 0\n",
    "    for match in re.finditer(r'^data:\\s*(.+?)(\\n\\n)', buffer):\n",
    "        try:\n",
    "            data = json.loads(match.group(1).strip())\n",
    "            last_idx = match.span()[1]\n",
    "            print(data[\"choices\"][0][\"delta\"][\"content\"], end=\"\")\n",
    "        except (json.JSONDecodeError, KeyError, IndexError) as e:\n",
    "            pass\n",
    "    buffer = buffer[last_idx:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3 Completion api non-stream mode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# from transformers import AutoTokenizer\n",
    "# tokenizer = AutoTokenizer.from_pretrained(\"deepseek-ai/deepseek-coder-6.7b-instruct\", trust_remote_code=True)\n",
    "# messages=[\n",
    "#     { 'role': 'user', 'content': \"write a quick sort algorithm in python.\"}\n",
    "# ]\n",
    "# prompt = tokenizer.apply_chat_template(messages, add_generation_prompt=True, tokenize=False)\n",
    "\n",
    "# payload = {\n",
    "#     \"model\": \"deepseek-ai/deepseek-coder-1.3b-instruct\",\n",
    "#     \"prompt\": prompt,\n",
    "#     \"max_tokens\": 1024,\n",
    "#     \"stream\": False\n",
    "# }\n",
    "endpoint_name = \"Llama-3-2-3B-Instruct-2025-02-23-13-01-30-887-sglang-endpoint\"\n",
    "payload = {\n",
    "    \"messages\": [\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": \"Write a quick sort in python\"\n",
    "    }\n",
    "    ],\n",
    "    \"model\":\"custome\",\n",
    "    \"max_tokens\": 1000,\n",
    "    \"stream\": False\n",
    "}\n",
    "\n",
    "response = runtime.invoke_endpoint(\n",
    "    EndpointName=endpoint_name,\n",
    "    ContentType='application/json',\n",
    "    Body=json.dumps(payload)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'id': 'd06be2c9d2964d24adec6eaa20dbd1a2', 'object': 'chat.completion', 'created': 1740368138, 'model': 'custome', 'choices': [{'index': 0, 'message': {'role': 'assistant', 'content': \"<think>\\nOkay, I need to write a quick sort algorithm in Python. Let me think about how quick sort works. I remember that quick sort is a divide-and-conquer algorithm. It usually works by choosing a 'pivot' element from the array and partitioning the other elements into two sub-arrays, according to whether they are less than or greater than the pivot. Then, recursively applying the same process to the sub-arrays.\\n\\nSo, the first step is to select a pivot. Common choices could be the first element, the last element, the median of the first, middle, and last elements, or the median of a subset. Let's pick the median of the first, last and middle elements as the pivot. This can help reduce the worst-case scenario.\\n\\nOnce the pivot is chosen, we partition the array. The partition step involves rearranging the elements such that all elements less than the pivot are on the left, and all elements greater than the pivot are on the right. We can use a two-pointer technique. One pointer starts at the left end, and another at the right end. We then move the pointers towards each other until they meet. Then swap the two pointers' elements with the pivot element.\\n\\nLet's start writing the code. I need to define the quick_sort function. The base case is when the array has one or zero elements, so it should return the array as is. Otherwise, select the pivot, partition the array, and recursively sort the sub-arrays.\\n\\nHere's a possible implementation:\\n\\ndef quick_sort(arr):\\n    if len(arr) <= 1:\\n        return arr\\n    pivot = median_of_three(arr[0], arr[len(arr)//2], arr[-1])\\n    left = [x for x in arr if x < pivot]\\n    middle = [x for x in arr if x == pivot]\\n    right = [x for x in arr if x > pivot]\\n    return quick_sort(left) + middle + quick_sort(right)\\n\\ndef median_of_three(a, b, c):\\n    # Choose the median of a, b, c\\n    return sorted([a, b, c])[1]\\n\\nWait, but the median_of_three function might not work correctly if the elements are not distinct. For example, if a, b, c are the same, sorted order would return a, b, c, but median is the middle one. However, in Python, sorted([a, b, c]) will return a sorted list, so the median would be the second element. But in the case where a, b, c are equal, sorted([a, b, c]) would be sorted but the order isn't affected, so the median is the middle value. So this works fine.\\n\\nBut let's further optimize this by implementing the partition step instead of using the median. Using the median can help reduce the worst-case time complexity, but the partition step is more common. Let's modify the quick_sort function to implement the partition step:\\n\\ndef quick_sort(arr):\\n    if len(arr) <= 1:\\n        return arr\\n    pivot = arr[len(arr)//2]\\n    # Partition the array\\n    left = [x for x in arr if x < pivot]\\n    middle = [x for x in arr if x == pivot]\\n    right = [x for x in arr if x > pivot]\\n    return quick_sort(left) + middle + quick_sort(right)\\n\\nBut wait, the partition step is incomplete. The correct partition step should use two pointers. Let me try to implement that correctly.\\n\\ndef partition(arr, low, high):\\n    pivot = arr[high]\\n    i = low - 1\\n    for j in range(low, high):\\n        if arr[j] <= pivot:\\n            i += 1\\n            arr[i], arr[j] = arr[j], arr[i]\\n    arr[i+1], arr[high] = arr[high], arr[i+1]\\n    return i + 1\\n\\nThen the quick_sort function would call itself recursively on the left and right sub-arrays.\\n\\ndef quick_sort(arr, low=None, high=None):\\n    if low is None or high is None:\\n        low = 0\\n        high = len(arr) - 1\\n    if low < high:\\n        pi = partition(arr, low, high)\\n        quick_sort(arr, low, pi-1)\\n        quick_sort(arr, pi+1, high)\\n\\nBut wait, this way is called in-place partition, but the quick_sort function is also supposed to be in-place. However, in the previous example, the quick_sort function didn't use low and high parameters, which would lead to infinite recursion. Let me check the code again.\\n\\nIn the standard quick sort implementation, the partition function rearranges the elements and returns the index of the pivot. The quick_sort function then recursively calls itself on the left and right sub-arrays. To make\", 'tool_calls': None}, 'logprobs': None, 'finish_reason': 'length', 'matched_stop': None}], 'usage': {'prompt_tokens': 41, 'total_tokens': 1041, 'completion_tokens': 1000, 'prompt_tokens_details': None}}\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(json.loads(response['Body'].read()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "65.78947368421053"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "1000/15.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "endpoint_name = \"Llama-3-2-3B-Instruct-2025-02-24-02-47-43-275-vllm-endpoint\"\n",
    "\n",
    "payload = {\n",
    "    \"messages\": [\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": \"Write a quick sort in python\"\n",
    "    }\n",
    "    ],\n",
    "    \"model\":\"custome\",\n",
    "    \"max_tokens\": 1000,\n",
    "    \"stream\": False\n",
    "}\n",
    "\n",
    "response = runtime.invoke_endpoint(\n",
    "    EndpointName=endpoint_name,\n",
    "    ContentType='application/json',\n",
    "    Body=json.dumps(payload)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'id': 'chatcmpl-e8a5a693408340bb967859a3718baeca', 'object': 'chat.completion', 'created': 1740368084, 'model': '/tmp/model_file/', 'choices': [{'index': 0, 'message': {'role': 'assistant', 'reasoning_content': None, 'content': \"<think>\\nOkay, so I need to write a quick sort algorithm in Python. Let's start by recalling what quick sort is. It's a divide-and-conquer algorithm that works by selecting a 'pivot' element from the array and partitioning the other elements into two sub-arrays, according to whether they are less than or greater than the pivot. Then, recursively applying this process to the sub-arrays.\\n\\nI should first choose a pivot, but the choice of pivot can affect performance.通常的做法是用中间元素或者随机元素作为pivot.为了简单起见，这里用第一项或者最后一项作为pivot。比如，可能选择stack[len(stack)-1]或stack[0]。或者用户可能需要自定义的分割方式？\\n\\n先写一个基础的实现。比如，假设有一个排序好的函数来帮助处理重复元素的话，但通常是O(n)的时间复杂度，所以应该处理重复的情况。但这里可能先不考虑重复，之后考虑是否能优化。\\n\\n首先定义quick_sort函数。参数是数组list，要等于至少两个元素的情况。\\n\\n那步骤大概是这样的：\\n\\n1. 如果数组长度小于等于1，直接返回，因为已经排序好了。\\n2. 选择一个pivot。\\n\\n比如，假设选择最后一个元素作为pivot。然后 partition 子函数将数组重新排列，使得pivot所在的位置大于移动的元素部分，同时pivot处于正确的位置。\\n\\npartition方法的原理是将数组分成两部分，一部分比pivot小，一部分比pivot大。这时候pivot的位置应该位于其最终的位置。\\n\\n然后递归地对左右两边分别进行排序，直到整个数组排序完成。\\n\\n那分解步骤：\\n\\n首先定义partition子函数。比如，partition方法会交换某些元素，使得pivot位于其中某个位置，然后标记pivot作为栈顶或者当前的正确位置？\\n\\n例如，在Python中可以用等号来交换。假设数组list，要交换pivot和某个元素的位置。比如，最初pivot可能设为list[-1]或者0。然后partition子函数遍历数组，交换i元素，使得元素小于等于pivot的都要和pivot的位置左边交换，或者反之？\\n\\n或者，可能更简单的是选择一个pivot，然后将比pivot大的元素都移到右边。例如，把pivot放在末尾，然后把每个比pivot小的元素都交换到左边的某个位置，从而使得pivot的位置在子数组的正确位置。\\n\\n比如：\\n\\n在Python中，比如 list = [3,6,8,10,1,2,5,4], 选择6作为pivot。然后交换使得所有位数比6大的放右边，或者反过来？需要再仔细想想。\\n\\n正确的partition方法的逻辑应该是：\\n\\n将pivot放在末端，然后将数组分成左右两部分，左边都是比pivot小的，右边都是比pivot大的？或者可能我混淆了。比如正确的操作是将同一个pivot放在正确的位置，所以步骤可能是：\\n\\n选择pivot，然后遍历数组，将比pivot大的和pivot分开，并交换它们的位置。或者这样？\\n\\n例如，pivot的位置是j。i从0到j-1遍历，比较arr[i]和pivot，如果arr[i]<pivot，那么进行双重交换（swap(i,j)，即j的位置被i镜像交换，当i的元素比pivot小时）。\\n\\n或者可能保留pivot的位置，然后将i大于pivot的元素排序。\\n\\n这个时候，可能比target(pivot)的位置位于其最终的正确位置？\\n\\n或者，另一种思路是：\\n\\n将数组二分：比pivot大的放右边，比pivot小的放左边。例如，将数组分为左右两部分，其中左右部分的每一个元素都不大于pivot，或者大于等于pivot？\\n\\n举个例子，假设pivot是10，数组是 [3,4,5,7,10,12]\\n\\n在partition的时候，将所有大于/等于10的元素放在右边？或者比pivot小的放在左边。或者可能是把所有大于pivot的放到右边，而小于等于的放左边，可能需要交换。\\n\\n现在，正确的partition应该将pivot的位置放在正确的位置。例如：\\n\\n初始pivot为last元素。然后i=0，遍历数组：\\n\\n对于每个i，比较arr[i]和arr[last]，如果这个元素比arr[last]小，那么交换i和last的位置。或者相反\", 'tool_calls': []}, 'logprobs': None, 'finish_reason': 'length', 'stop_reason': None}], 'usage': {'prompt_tokens': 41, 'total_tokens': 1041, 'completion_tokens': 1000, 'prompt_tokens_details': None}, 'prompt_logprobs': None}\n"
     ]
    }
   ],
   "source": [
    "print(json.loads(response['Body'].read()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "62.893081761006286"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "1000/15.9"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.4 Completion api stream mode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# 指数回退重试 --------------\n",
      "Inference Component has no capacity, retrying in 10 seconds (attempt 1/8)\n",
      "Inference Component has no capacity, retrying in 20 seconds (attempt 2/8)\n",
      "Inference Component has no capacity, retrying in 40 seconds (attempt 3/8)\n",
      "Inference Component has no capacity, retrying in 80 seconds (attempt 4/8)\n",
      "Inference Component has no capacity, retrying in 160 seconds (attempt 5/8)\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValidationError\u001b[0m                           Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 30\u001b[0m, in \u001b[0;36minvoke_with_exponential_backoff\u001b[0;34m(endpoint_name, payload, retry_delay)\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 30\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[43mruntime\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minvoke_endpoint\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     31\u001b[0m \u001b[43m        \u001b[49m\u001b[43mEndpointName\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mendpoint_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     32\u001b[0m \u001b[43m        \u001b[49m\u001b[43mInferenceComponentName\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minference_component_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     33\u001b[0m \u001b[43m        \u001b[49m\u001b[43mContentType\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mapplication/json\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     34\u001b[0m \u001b[43m        \u001b[49m\u001b[43mBody\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mjson\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdumps\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpayload\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     35\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     36\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m response\n",
      "File \u001b[0;32m~/workspace/llm_model_hub/miniconda3/envs/py311/lib/python3.11/site-packages/botocore/client.py:570\u001b[0m, in \u001b[0;36mClientCreator._create_api_method.<locals>._api_call\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    569\u001b[0m \u001b[38;5;66;03m# The \"self\" in this scope is referring to the BaseClient.\u001b[39;00m\n\u001b[0;32m--> 570\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_make_api_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43moperation_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/workspace/llm_model_hub/miniconda3/envs/py311/lib/python3.11/site-packages/botocore/context.py:124\u001b[0m, in \u001b[0;36mwith_current_context.<locals>.decorator.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    123\u001b[0m     hook()\n\u001b[0;32m--> 124\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/workspace/llm_model_hub/miniconda3/envs/py311/lib/python3.11/site-packages/botocore/client.py:1031\u001b[0m, in \u001b[0;36mBaseClient._make_api_call\u001b[0;34m(self, operation_name, api_params)\u001b[0m\n\u001b[1;32m   1030\u001b[0m     error_class \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mexceptions\u001b[38;5;241m.\u001b[39mfrom_code(error_code)\n\u001b[0;32m-> 1031\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m error_class(parsed_response, operation_name)\n\u001b[1;32m   1032\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[0;31mValidationError\u001b[0m: An error occurred (ValidationError) when calling the InvokeEndpoint operation: Inference Component has no capacity to process this request. ApplicationAutoScaling may be in-progress (if configured) or try to increase the capacity by invoking UpdateInferenceComponentRuntimeConfig API.",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 48\u001b[0m\n\u001b[1;32m     45\u001b[0m             \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     46\u001b[0m                 \u001b[38;5;28;01mraise\u001b[39;00m\n\u001b[0;32m---> 48\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[43minvoke_with_exponential_backoff\u001b[49m\u001b[43m(\u001b[49m\u001b[43mendpoint_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpayload\u001b[49m\u001b[43m,\u001b[49m\u001b[43mretry_delay\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     49\u001b[0m \u001b[38;5;28mprint\u001b[39m(json\u001b[38;5;241m.\u001b[39mloads(response[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mBody\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mread())[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mchoices\u001b[39m\u001b[38;5;124m\"\u001b[39m][\u001b[38;5;241m0\u001b[39m][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmessage\u001b[39m\u001b[38;5;124m\"\u001b[39m][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcontent\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[1;32m     52\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m# 非流式 --------------\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[0;32mIn[1], line 41\u001b[0m, in \u001b[0;36minvoke_with_exponential_backoff\u001b[0;34m(endpoint_name, payload, retry_delay)\u001b[0m\n\u001b[1;32m     39\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m attempt \u001b[38;5;241m<\u001b[39m max_retries \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m     40\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInference Component has no capacity, retrying in \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mretry_delay\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m seconds (attempt \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mattempt\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmax_retries\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m)\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 41\u001b[0m     \u001b[43mtime\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msleep\u001b[49m\u001b[43m(\u001b[49m\u001b[43mretry_delay\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     42\u001b[0m     retry_delay \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m2\u001b[39m  \u001b[38;5;66;03m# Exponential backoff\u001b[39;00m\n\u001b[1;32m     43\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import re\n",
    "import json\n",
    "import boto3\n",
    "from botocore.exceptions import ClientError\n",
    "import time\n",
    "max_retries = 8\n",
    "retry_delay = 10\n",
    "\n",
    "# 更改成model hub部署的endpoint名称和region name\n",
    "endpoint_name = \"Qwen2-5-VL-3B-Instruct-2025-03-07-15-34-sglang-endpoint\"\n",
    "inference_component_name = \"Qwen2-5-VL-3B-Instruct-2025-03-07-15-34-sglang-component\"\n",
    "region_name = 'us-east-1'\n",
    "runtime = boto3.client('runtime.sagemaker',region_name=region_name)\n",
    "payload = {\n",
    "    \"messages\": [\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": \"who are you\"\n",
    "    }\n",
    "    ],\n",
    "    \"max_tokens\": 1024,\n",
    "    \"stream\": False,\n",
    "    \"model\":\"any\"\n",
    "}\n",
    "\n",
    "print(\"# 指数回退重试 --------------\")\n",
    "def invoke_with_exponential_backoff(endpoint_name, payload,retry_delay):\n",
    "    for attempt in range(max_retries):\n",
    "        try:\n",
    "            response = runtime.invoke_endpoint(\n",
    "                EndpointName=endpoint_name,\n",
    "                InferenceComponentName=inference_component_name,\n",
    "                ContentType='application/json',\n",
    "                Body=json.dumps(payload)\n",
    "            )\n",
    "            return response\n",
    "        except ClientError as e:\n",
    "            if e.response['Error']['Code'] == 'ValidationError' and 'Inference Component has no capacity' in e.response['Error']['Message']:\n",
    "                if attempt < max_retries - 1:\n",
    "                    print(f\"Inference Component has no capacity, retrying in {retry_delay} seconds (attempt {attempt+1}/{max_retries})\")\n",
    "                    time.sleep(retry_delay)\n",
    "                    retry_delay *= 2  # Exponential backoff\n",
    "                else:\n",
    "                    print(\"Maximum retries reached, unable to process request\")\n",
    "            else:\n",
    "                raise\n",
    "            \n",
    "response = invoke_with_exponential_backoff(endpoint_name, payload,retry_delay)\n",
    "print(json.loads(response['Body'].read())[\"choices\"][0][\"message\"][\"content\"])\n",
    "\n",
    "\n",
    "print(\"# 非流式 --------------\")\n",
    "# 非流式\n",
    "response = runtime.invoke_endpoint(\n",
    "    EndpointName=endpoint_name,\n",
    "    InferenceComponentName=inference_component_name,\n",
    "    ContentType='application/json',\n",
    "    Body=json.dumps(payload)\n",
    ")\n",
    "\n",
    "print(json.loads(response['Body'].read())[\"choices\"][0][\"message\"][\"content\"])\n",
    "\n",
    "\n",
    "payload = {\n",
    "    \"messages\": [\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": \"Write a quick sort in python\"\n",
    "    }\n",
    "    ],\n",
    "    \"max_tokens\": 1024,\n",
    "    \"stream\": True,\n",
    "    \"model\":\"any\"\n",
    "}\n",
    "\n",
    "\n",
    "# 流式\n",
    "print(\"# 流式 --------------\")\n",
    "response = runtime.invoke_endpoint_with_response_stream(\n",
    "    EndpointName=endpoint_name,\n",
    "    InferenceComponentName=inference_component_name,\n",
    "    ContentType='application/json',\n",
    "    Body=json.dumps(payload)\n",
    ")\n",
    "\n",
    "buffer = \"\"\n",
    "for t in response['Body']:\n",
    "    buffer += t[\"PayloadPart\"][\"Bytes\"].decode()\n",
    "    last_idx = 0\n",
    "    for match in re.finditer(r'^data:\\s*(.+?)(\\n\\n)', buffer):\n",
    "        try:\n",
    "            data = json.loads(match.group(1).strip())\n",
    "            last_idx = match.span()[1]\n",
    "            print(data[\"choices\"][0][\"delta\"][\"content\"], end=\"\",flush=True)\n",
    "        except (json.JSONDecodeError, KeyError, IndexError) as e:\n",
    "            pass\n",
    "    buffer = buffer[last_idx:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py311",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
