# SGLang v0.5.8 Prefill-Decode Disaggregated Deployment for Qwen3-4B-Instruct-2507
# Cluster: modelhub16-eks (2x ml.g6.12xlarge)
# Status: VERIFIED WORKING (2026-01-31)
#
# Architecture:
#   Client -> Router (sglang-router) -> Prefill Worker -> Mooncake -> Decode Worker
#                                              |                           |
#                                              +---- KV Cache Transfer ----+
#                                                        (TCP)
#
# Components:
#   1. Prefill Worker: Handles prompt processing, generates KV cache
#   2. Decode Worker: Handles token generation using transferred KV cache
#   3. Router (sglang-router): PD-aware load balancer that coordinates prefill and decode
#
# Notes:
#   - EFA device plugin not installed on g6.12xlarge, using TCP transport for Mooncake
#   - Router may show tokenizer loading warnings - these are non-blocking, router still works
#     because workers handle tokenization
#
# Usage:
#   KUBECONFIG=/home/ubuntu/.kube/config-modelhub16-eks kubectl apply -f sglang-pd-disaggregated-qwen3-4b.yaml
#
# Verify:
#   KUBECONFIG=/home/ubuntu/.kube/config-modelhub16-eks kubectl get pods -l app=sglang-pd
#   KUBECONFIG=/home/ubuntu/.kube/config-modelhub16-eks kubectl logs -l role=router -f
#
# Test:
#   KUBECONFIG=/home/ubuntu/.kube/config-modelhub16-eks kubectl port-forward svc/sglang-router-service 8000:8000
#   curl http://localhost:8000/v1/chat/completions -H "Content-Type: application/json" \
#     -d '{"model": "Qwen3-4B-Instruct-2507", "messages": [{"role": "user", "content": "Hello"}]}'
#
# Cleanup:
#   KUBECONFIG=/home/ubuntu/.kube/config-modelhub16-eks kubectl delete -f sglang-pd-disaggregated-qwen3-4b.yaml

---
# Prefill Worker Deployment
apiVersion: apps/v1
kind: Deployment
metadata:
  name: sglang-prefill
  namespace: default
  labels:
    app: sglang-pd
    role: prefill
spec:
  replicas: 1
  selector:
    matchLabels:
      app: sglang-pd
      role: prefill
  template:
    metadata:
      labels:
        app: sglang-pd
        role: prefill
    spec:
      containers:
      - name: sglang-prefill
        image: lmsysorg/sglang:v0.5.8
        command:
        - sh
        - -c
        - |
          echo "=========================================="
          echo "Starting SGLang v0.5.8 Prefill Worker"
          echo "=========================================="
          echo "Model path: /mnt/s3/Qwen3-4B-Instruct-2507/0d0f748bad7a4aa7a3cae5ec42bb4553/finetuned_model_merged/"
          echo "Checking model files..."
          ls -la /mnt/s3/Qwen3-4B-Instruct-2507/0d0f748bad7a4aa7a3cae5ec42bb4553/finetuned_model_merged/ || echo "Model path not accessible yet, will retry..."

          echo "Starting prefill server..."
          python3 -m sglang.launch_server \
            --model-path /mnt/s3/Qwen3-4B-Instruct-2507/0d0f748bad7a4aa7a3cae5ec42bb4553/finetuned_model_merged/ \
            --served-model-name Qwen3-4B-Instruct-2507 \
            --tp 4 \
            --host 0.0.0.0 \
            --port 30000 \
            --disaggregation-mode prefill \
            --disaggregation-transfer-backend mooncake \
            --disaggregation-bootstrap-port 8998 \
            --trust-remote-code \
            --enable-metrics \
            --mem-fraction-static 0.85 \
            --chunked-prefill-size 4096 \
            --context-length 32768 \
            --log-level info

        env:
        - name: CUDA_VISIBLE_DEVICES
          value: "0,1,2,3"
        - name: NCCL_DEBUG
          value: "WARN"
        # Mooncake TCP transport config (EFA not available)
        - name: MC_GID_INDEX
          value: "0"
        - name: MOONCAKE_TRANSPORT
          value: "tcp"

        resources:
          limits:
            nvidia.com/gpu: 4
            cpu: "48"
            memory: "192Gi"
          requests:
            nvidia.com/gpu: 4
            cpu: "24"
            memory: "96Gi"

        ports:
        - name: http
          containerPort: 30000
          protocol: TCP
        - name: bootstrap
          containerPort: 8998
          protocol: TCP
        - name: metrics
          containerPort: 9090
          protocol: TCP

        volumeMounts:
        - name: s3-model
          mountPath: /mnt/s3
          readOnly: true
        - name: shm
          mountPath: /dev/shm

        livenessProbe:
          httpGet:
            path: /health
            port: 30000
          initialDelaySeconds: 300
          periodSeconds: 30
          timeoutSeconds: 10
          failureThreshold: 5

        readinessProbe:
          httpGet:
            path: /health
            port: 30000
          initialDelaySeconds: 300
          periodSeconds: 10
          timeoutSeconds: 5
          failureThreshold: 3

      volumes:
      - name: s3-model
        persistentVolumeClaim:
          claimName: s3-pvc
      - name: shm
        emptyDir:
          medium: Memory
          sizeLimit: 32Gi

      nodeSelector:
        node.kubernetes.io/instance-type: ml.g6.12xlarge

      tolerations:
      - key: nvidia.com/gpu
        operator: Exists
        effect: NoSchedule

      # Anti-affinity: prefill and decode must run on different nodes
      affinity:
        podAntiAffinity:
          requiredDuringSchedulingIgnoredDuringExecution:
          - labelSelector:
              matchLabels:
                app: sglang-pd
                role: decode
            topologyKey: kubernetes.io/hostname

---
# Decode Worker Deployment
apiVersion: apps/v1
kind: Deployment
metadata:
  name: sglang-decode
  namespace: default
  labels:
    app: sglang-pd
    role: decode
spec:
  replicas: 1
  selector:
    matchLabels:
      app: sglang-pd
      role: decode
  template:
    metadata:
      labels:
        app: sglang-pd
        role: decode
    spec:
      containers:
      - name: sglang-decode
        image: lmsysorg/sglang:v0.5.8
        command:
        - sh
        - -c
        - |
          echo "=========================================="
          echo "Starting SGLang v0.5.8 Decode Worker"
          echo "=========================================="
          echo "Model path: /mnt/s3/Qwen3-4B-Instruct-2507/0d0f748bad7a4aa7a3cae5ec42bb4553/finetuned_model_merged/"
          echo "Checking model files..."
          ls -la /mnt/s3/Qwen3-4B-Instruct-2507/0d0f748bad7a4aa7a3cae5ec42bb4553/finetuned_model_merged/ || echo "Model path not accessible yet, will retry..."

          echo "Starting decode server..."
          python3 -m sglang.launch_server \
            --model-path /mnt/s3/Qwen3-4B-Instruct-2507/0d0f748bad7a4aa7a3cae5ec42bb4553/finetuned_model_merged/ \
            --served-model-name Qwen3-4B-Instruct-2507 \
            --tp 4 \
            --host 0.0.0.0 \
            --port 30001 \
            --disaggregation-mode decode \
            --disaggregation-transfer-backend mooncake \
            --trust-remote-code \
            --enable-metrics \
            --mem-fraction-static 0.85 \
            --max-running-requests 256 \
            --disable-radix-cache \
            --log-level info

        env:
        - name: CUDA_VISIBLE_DEVICES
          value: "0,1,2,3"
        - name: NCCL_DEBUG
          value: "WARN"
        # Mooncake TCP transport config (EFA not available)
        - name: MC_GID_INDEX
          value: "0"
        - name: MOONCAKE_TRANSPORT
          value: "tcp"

        resources:
          limits:
            nvidia.com/gpu: 4
            cpu: "48"
            memory: "192Gi"
          requests:
            nvidia.com/gpu: 4
            cpu: "24"
            memory: "96Gi"

        ports:
        - name: http
          containerPort: 30001
          protocol: TCP
        - name: metrics
          containerPort: 9090
          protocol: TCP

        volumeMounts:
        - name: s3-model
          mountPath: /mnt/s3
          readOnly: true
        - name: shm
          mountPath: /dev/shm

        livenessProbe:
          httpGet:
            path: /health
            port: 30001
          initialDelaySeconds: 300
          periodSeconds: 30
          timeoutSeconds: 10
          failureThreshold: 5

        readinessProbe:
          httpGet:
            path: /health
            port: 30001
          initialDelaySeconds: 300
          periodSeconds: 10
          timeoutSeconds: 5
          failureThreshold: 3

      volumes:
      - name: s3-model
        persistentVolumeClaim:
          claimName: s3-pvc
      - name: shm
        emptyDir:
          medium: Memory
          sizeLimit: 32Gi

      nodeSelector:
        node.kubernetes.io/instance-type: ml.g6.12xlarge

      tolerations:
      - key: nvidia.com/gpu
        operator: Exists
        effect: NoSchedule

      # Anti-affinity: decode and prefill must run on different nodes
      affinity:
        podAntiAffinity:
          requiredDuringSchedulingIgnoredDuringExecution:
          - labelSelector:
              matchLabels:
                app: sglang-pd
                role: prefill
            topologyKey: kubernetes.io/hostname

---
# Router (mini_lb) Deployment
apiVersion: apps/v1
kind: Deployment
metadata:
  name: sglang-router
  namespace: default
  labels:
    app: sglang-pd
    role: router
spec:
  replicas: 1
  selector:
    matchLabels:
      app: sglang-pd
      role: router
  template:
    metadata:
      labels:
        app: sglang-pd
        role: router
    spec:
      containers:
      - name: sglang-router
        image: lmsysorg/sglang:v0.5.8
        command:
        - sh
        - -c
        - |
          echo "=========================================="
          echo "Starting SGLang v0.5.8 Router"
          echo "=========================================="
          echo "Installing sglang-router package..."
          pip install sglang-router -q

          echo "Waiting for prefill and decode workers to be ready..."
          sleep 60

          echo "Starting router..."
          python3 -m sglang_router.launch_router \
            --host 0.0.0.0 \
            --port 8000 \
            --pd-disaggregation \
            --prefill http://sglang-prefill-service:30000 \
            --decode http://sglang-decode-service:30001

        resources:
          limits:
            cpu: "4"
            memory: "8Gi"
          requests:
            cpu: "2"
            memory: "4Gi"

        ports:
        - name: http
          containerPort: 8000
          protocol: TCP

        livenessProbe:
          httpGet:
            path: /health
            port: 8000
          initialDelaySeconds: 60
          periodSeconds: 30
          timeoutSeconds: 10
          failureThreshold: 5

        readinessProbe:
          httpGet:
            path: /health
            port: 8000
          initialDelaySeconds: 60
          periodSeconds: 10
          timeoutSeconds: 5
          failureThreshold: 3

      # Router can run on any node (no GPU needed)
      tolerations:
      - key: nvidia.com/gpu
        operator: Exists
        effect: NoSchedule

---
# Prefill Worker Service (internal, for router and decode communication)
apiVersion: v1
kind: Service
metadata:
  name: sglang-prefill-service
  namespace: default
  labels:
    app: sglang-pd
    role: prefill
spec:
  selector:
    app: sglang-pd
    role: prefill
  ports:
  - name: http
    protocol: TCP
    port: 30000
    targetPort: 30000
  - name: bootstrap
    protocol: TCP
    port: 8998
    targetPort: 8998
  - name: metrics
    protocol: TCP
    port: 9090
    targetPort: 9090
  type: ClusterIP

---
# Decode Worker Service (internal, for router and prefill communication)
apiVersion: v1
kind: Service
metadata:
  name: sglang-decode-service
  namespace: default
  labels:
    app: sglang-pd
    role: decode
spec:
  selector:
    app: sglang-pd
    role: decode
  ports:
  - name: http
    protocol: TCP
    port: 30001
    targetPort: 30001
  - name: metrics
    protocol: TCP
    port: 9090
    targetPort: 9090
  type: ClusterIP

---
# Router Service (main entry point for inference requests)
apiVersion: v1
kind: Service
metadata:
  name: sglang-router-service
  namespace: default
  labels:
    app: sglang-pd
    role: router
  annotations:
    prometheus.io/scrape: "true"
    prometheus.io/port: "8000"
spec:
  selector:
    app: sglang-pd
    role: router
  ports:
  - name: http
    protocol: TCP
    port: 8000
    targetPort: 8000
  type: ClusterIP
